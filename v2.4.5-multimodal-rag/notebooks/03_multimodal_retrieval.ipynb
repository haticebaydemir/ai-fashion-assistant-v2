{"cells":[{"cell_type":"markdown","metadata":{"id":"pnpOp2cKg81U"},"source":["# AI Fashion Assistant v2.4.5 - Multi-Modal RAG\n","\n","**Day 3: Multimodal Retrieval**\n","\n","---\n","\n","**Project:** AI Fashion Assistant (TÜBİTAK 2209-A)  \n","**Student:** Hatice Baydemir  \n","**Date:** January 8, 2026  \n","**Version:** 2.4.5\n","\n","---\n","\n","## Goal\n","\n","Implement multimodal retrieval system:\n","1. Load FAISS indices (text + image)\n","2. Build MultiModalRetriever class\n","3. Implement fusion strategies\n","4. Test 3 retrieval modes (text-only, image-only, multimodal)\n","5. Attribute-based filtering\n","6. Compare results\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"-NEu6tDWg81W"},"source":["## PART 1: Setup"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vegYXcIgg81W","executionInfo":{"status":"ok","timestamp":1767904689367,"user_tz":-180,"elapsed":2181,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"fc33aa61-5ea3-4174-eb3b-e090b75e4fee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive mounted\n","Working directory: /content/drive/MyDrive/ai_fashion_assistant_v2\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/MyDrive/ai_fashion_assistant_v2')\n","\n","print('Drive mounted')\n","print(f'Working directory: {os.getcwd()}')"]},{"cell_type":"code","execution_count":114,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdfF59CGg81X","executionInfo":{"status":"ok","timestamp":1767904689373,"user_tz":-180,"elapsed":8,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"5c94abb4-bb38-4267-d8da-c68b23a27d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Imports complete\n"]}],"source":["import json\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from pathlib import Path\n","from typing import Dict, List, Tuple, Optional\n","import matplotlib.pyplot as plt\n","import torch\n","from PIL import Image\n","\n","print('Imports complete')"]},{"cell_type":"markdown","metadata":{"id":"M35hZevcg81X"},"source":["---\n","\n","## PART 2: Load FAISS Indices"]},{"cell_type":"code","execution_count":115,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Off2qYZgg81X","executionInfo":{"status":"ok","timestamp":1767904704387,"user_tz":-180,"elapsed":15013,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"89e4b823-fcf2-4510-c406-9038cae7d68c"},"outputs":[{"output_type":"stream","name":"stdout","text":["FAISS installed\n"]}],"source":["# Install FAISS\n","!pip install -q faiss-cpu\n","\n","import faiss\n","\n","print('FAISS installed')"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gnyx824og81X","executionInfo":{"status":"ok","timestamp":1767904707158,"user_tz":-180,"elapsed":2768,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"4214e19f-651b-44c1-c164-211b94c82961"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ FAISS index loaded: 44417 vectors, 2304 dimensions\n"]}],"source":["# Load FAISS index (hybrid)\n","faiss_index_path = 'indexes/faiss_hybrid_hnsw.index'\n","\n","if Path(faiss_index_path).exists():\n","    faiss_index = faiss.read_index(faiss_index_path)\n","    print(f'✓ FAISS index loaded: {faiss_index.ntotal} vectors, {faiss_index.d} dimensions')\n","else:\n","    print(f'✗ FAISS index not found')\n","    faiss_index = None"]},{"cell_type":"code","execution_count":117,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOvnnh-0g81X","executionInfo":{"status":"ok","timestamp":1767904707181,"user_tz":-180,"elapsed":21,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"70b273bb-b108-42a4-ec65-715daa3a1a92"},"outputs":[{"output_type":"stream","name":"stdout","text":["✗ Image index not found at v2.1-core-ml-plus/embeddings/faiss_index_clip.index\n","\n","Will create image index if needed...\n"]}],"source":["# Check if image FAISS index exists (from v2.1)\n","image_index_path = 'v2.1-core-ml-plus/embeddings/faiss_index_clip.index'\n","\n","if Path(image_index_path).exists():\n","    image_index = faiss.read_index(image_index_path)\n","    print(f'✓ Image index loaded: {image_index.ntotal} vectors, {image_index.d} dimensions')\n","else:\n","    print(f'✗ Image index not found at {image_index_path}')\n","    print('\\nWill create image index if needed...')\n","    image_index = None"]},{"cell_type":"markdown","metadata":{"id":"0Zjr4DCzg81Y"},"source":["---\n","\n","## PART 3: Load Product Metadata & Embeddings"]},{"cell_type":"code","execution_count":118,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUSQlruqg81Y","executionInfo":{"status":"ok","timestamp":1767904707735,"user_tz":-180,"elapsed":539,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"88d388bf-ea27-4917-9de6-b4a4a6a89315"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 44417 products\n","Columns: ['id', 'productDisplayName', 'masterCategory', 'subCategory', 'articleType', 'baseColour', 'gender', 'season', 'year', 'usage', 'desc', 'image_path', 'text_embedding', 'image_embedding', 'hybrid_embedding']\n"]}],"source":["# Load product metadata\n","products_df = pd.read_csv('data/processed/meta_ssot.csv')\n","\n","print(f'Loaded {len(products_df)} products')\n","print(f'Columns: {list(products_df.columns)}')"]},{"cell_type":"code","execution_count":119,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LqlFt3Gsg81Y","executionInfo":{"status":"ok","timestamp":1767904708229,"user_tz":-180,"elapsed":486,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"9afa166e-a88f-4b6d-f5da-dbe76854f5e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Text embeddings loaded: (44417, 768)\n"]}],"source":["# Load text embeddings (mpnet 768d)\n","text_emb_path = 'v2.0-baseline/embeddings/text/mpnet_768d.npy'\n","\n","if Path(text_emb_path).exists():\n","    text_embeddings = np.load(text_emb_path)\n","    print(f'✓ Text embeddings loaded: {text_embeddings.shape}')\n","else:\n","    print(f'✗ Text embeddings not found')\n","    text_embeddings = None"]},{"cell_type":"code","source":["# Load image embeddings (CLIP 768d)\n","image_emb_path = 'v2.0-baseline/embeddings/image/clip_image_768d.npy'\n","\n","if Path(image_emb_path).exists():\n","    image_embeddings = np.load(image_emb_path)\n","    print(f'✓ Image embeddings loaded: {image_embeddings.shape}')\n","else:\n","    print(f'✗ Image embeddings not found')\n","    image_embeddings = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yzof5B2-kzsA","executionInfo":{"status":"ok","timestamp":1767904708937,"user_tz":-180,"elapsed":706,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"10402e7e-7e02-4722-d542-f4c259bda04a"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["✓ Image embeddings loaded: (44417, 768)\n"]}]},{"cell_type":"code","source":["# Load CLIP model for encoding queries\n","from transformers import CLIPProcessor, CLIPModel\n","\n","model_name = \"openai/clip-vit-large-patch14\"\n","print(f'Loading CLIP model: {model_name}')\n","model = CLIPModel.from_pretrained(model_name)\n","processor = CLIPProcessor.from_pretrained(model_name)\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = model.to(device)\n","\n","print(f'✓ CLIP model loaded on {device}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5CeiAosZMuMc","executionInfo":{"status":"ok","timestamp":1767904719290,"user_tz":-180,"elapsed":10348,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"a5f508bc-4a59-4699-fee9-c215012186b2"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading CLIP model: openai/clip-vit-large-patch14\n","✓ CLIP model loaded on cpu\n"]}]},{"cell_type":"code","execution_count":122,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TqQXT3qng81Y","executionInfo":{"status":"ok","timestamp":1767904719313,"user_tz":-180,"elapsed":11,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"dd4c5bfa-0059-4f06-8c24-74f9ffed6d08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding functions ready\n"]}],"source":["# Encoding functions\n","def encode_text_clip(text: str) -> np.ndarray:\n","    \"\"\"Encode text with CLIP (768d)\"\"\"\n","    inputs = processor(text=[text], return_tensors=\"pt\", padding=True).to(device)\n","    with torch.no_grad():\n","        outputs = model.get_text_features(**inputs)\n","    return outputs.cpu().numpy()[0]\n","\n","def encode_image_clip(image_path: str) -> np.ndarray:\n","    \"\"\"Encode image with CLIP (768d)\"\"\"\n","    image = Image.open(image_path).convert('RGB')\n","    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n","    with torch.no_grad():\n","        outputs = model.get_image_features(**inputs)\n","    return outputs.cpu().numpy()[0]\n","\n","print('Encoding functions ready')"]},{"cell_type":"markdown","metadata":{"id":"qb3kqmeZg81Y"},"source":["---\n","\n","## PART 4: MultiModalRetriever Class"]},{"cell_type":"code","execution_count":123,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVxpewm-g81Y","executionInfo":{"status":"ok","timestamp":1767904719401,"user_tz":-180,"elapsed":81,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"d759a4b9-ce85-4cd5-a9d6-f95d0a0e0782"},"outputs":[{"output_type":"stream","name":"stdout","text":["MultiModalRetriever class defined\n"]}],"source":["# PART 4: MultiModalRetriever Class\n","\n","class MultiModalRetriever:\n","    \"\"\"Multimodal retrieval with learned fusion\"\"\"\n","\n","    def __init__(self,\n","                 text_index: faiss.Index,\n","                 image_index: faiss.Index,\n","                 products_df: pd.DataFrame,\n","                 alpha: float = 0.7):\n","        \"\"\"\n","        Args:\n","            text_index: FAISS index for text embeddings\n","            image_index: FAISS index for image embeddings\n","            products_df: Product metadata\n","            alpha: Text weight in fusion (0.7 = 70% text, 30% image)\n","        \"\"\"\n","        self.text_index = text_index\n","        self.image_index = image_index\n","        self.products_df = products_df\n","        self.alpha = alpha\n","\n","    def retrieve_by_text(self, text_query: str, k: int = 20) -> Tuple[List[int], List[float]]:\n","        \"\"\"Text-only retrieval (baseline)\"\"\"\n","        # Encode query\n","        text_emb = encode_text_clip(text_query)\n","        text_emb = text_emb.reshape(1, -1).astype('float32')\n","\n","        # Search\n","        distances, indices = self.text_index.search(text_emb, k)\n","\n","        return indices[0].tolist(), distances[0].tolist()\n","\n","    def retrieve_by_image(self, image_path: str, k: int = 20) -> Tuple[List[int], List[float]]:\n","        \"\"\"Image-only retrieval (new capability)\"\"\"\n","        if self.image_index is None:\n","            return [], []\n","\n","        # Encode image\n","        img_emb = encode_image_clip(image_path)\n","        img_emb = img_emb.reshape(1, -1).astype('float32')\n","\n","        # Search\n","        distances, indices = self.image_index.search(img_emb, k)\n","\n","        return indices[0].tolist(), distances[0].tolist()\n","\n","    def retrieve_multimodal(self,\n","                           image_path: str = None,\n","                           text_query: str = None,\n","                           k: int = 20) -> List[int]:\n","        \"\"\"Multimodal fusion retrieval\"\"\"\n","\n","        # Get rankings from both modalities\n","        if image_path:\n","            img_indices, img_scores = self.retrieve_by_image(image_path, k=50)\n","        else:\n","            img_indices, img_scores = [], []\n","\n","        if text_query:\n","            text_indices, text_scores = self.retrieve_by_text(text_query, k=50)\n","        else:\n","            text_indices, text_scores = [], []\n","\n","        # Fusion strategy\n","        if not img_indices and not text_indices:\n","            return []\n","        elif not img_indices:\n","            return text_indices[:k]\n","        elif not text_indices:\n","            return img_indices[:k]\n","\n","        # Normalize scores to [0, 1]\n","        text_scores_norm = self._normalize_scores(text_scores)\n","        img_scores_norm = self._normalize_scores(img_scores)\n","\n","        # Create score maps\n","        text_score_map = {idx: score for idx, score in zip(text_indices, text_scores_norm)}\n","        img_score_map = {idx: score for idx, score in zip(img_indices, img_scores_norm)}\n","\n","        # Combine all product IDs\n","        all_products = set(text_indices) | set(img_indices)\n","\n","        # Fusion: weighted combination\n","        fused_scores = {}\n","        for prod_id in all_products:\n","            text_score = text_score_map.get(prod_id, 0)\n","            img_score = img_score_map.get(prod_id, 0)\n","            fused_scores[prod_id] = self.alpha * text_score + (1 - self.alpha) * img_score\n","\n","        # Sort by fused score\n","        ranked = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","        return [prod_id for prod_id, score in ranked[:k]]\n","\n","    def _normalize_scores(self, scores: List[float]) -> List[float]:\n","        \"\"\"Normalize scores to [0, 1] using min-max\"\"\"\n","        if not scores:\n","            return []\n","\n","        scores = np.array(scores)\n","\n","        # FAISS L2 distance: smaller distance = more similar\n","        # Negatif yap: küçük distance = büyük score\n","        similarities = -scores\n","\n","        # Min-max normalize to [0, 1]\n","        min_sim = similarities.min()\n","        max_sim = similarities.max()\n","\n","        if max_sim - min_sim > 0:\n","            normalized = (similarities - min_sim) / (max_sim - min_sim)\n","        else:\n","            normalized = np.ones_like(similarities)\n","\n","        return normalized.tolist()\n","\n","print('MultiModalRetriever class defined')"]},{"cell_type":"markdown","metadata":{"id":"y6K_8M5og81Z"},"source":["---\n","\n","## PART 5: Initialize Retriever"]},{"cell_type":"code","execution_count":124,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hjZYC828g81Z","executionInfo":{"status":"ok","timestamp":1767908072434,"user_tz":-180,"elapsed":3353022,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"e28ec8e9-f7c1-46a8-f760-9268eae2a07b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating CLIP text embeddings for all products...\n","This will take ~10 minutes for 44,417 products\n","  Processed 1000/44417 products...\n","  Processed 2000/44417 products...\n","  Processed 3000/44417 products...\n","  Processed 4000/44417 products...\n","  Processed 5000/44417 products...\n","  Processed 6000/44417 products...\n","  Processed 7000/44417 products...\n","  Processed 8000/44417 products...\n","  Processed 9000/44417 products...\n","  Processed 10000/44417 products...\n","  Processed 11000/44417 products...\n","  Processed 12000/44417 products...\n","  Processed 13000/44417 products...\n","  Processed 14000/44417 products...\n","  Processed 15000/44417 products...\n","  Processed 16000/44417 products...\n","  Processed 17000/44417 products...\n","  Processed 18000/44417 products...\n","  Processed 19000/44417 products...\n","  Processed 20000/44417 products...\n","  Processed 21000/44417 products...\n","  Processed 22000/44417 products...\n","  Processed 23000/44417 products...\n","  Processed 24000/44417 products...\n","  Processed 25000/44417 products...\n","  Processed 26000/44417 products...\n","  Processed 27000/44417 products...\n","  Processed 28000/44417 products...\n","  Processed 29000/44417 products...\n","  Processed 30000/44417 products...\n","  Processed 31000/44417 products...\n","  Processed 32000/44417 products...\n","  Processed 33000/44417 products...\n","  Processed 34000/44417 products...\n","  Processed 35000/44417 products...\n","  Processed 36000/44417 products...\n","  Processed 37000/44417 products...\n","  Processed 38000/44417 products...\n","  Processed 39000/44417 products...\n","  Processed 40000/44417 products...\n","  Processed 41000/44417 products...\n","  Processed 42000/44417 products...\n","  Processed 43000/44417 products...\n","  Processed 44000/44417 products...\n","✓ CLIP text embeddings created: (44417, 768)\n","✓ Text index created with CLIP: 44417 vectors\n","✓ Image index created: 44417 vectors\n","✓ Created index-to-ID mapping: 44417 products\n","✓ MultiModalRetriever initialized with CLIP text embeddings\n","  Alpha (text weight): 0.7\n","  Text: CLIP, Image: CLIP → Consistent embedding space!\n"]}],"source":["# PART 5: Create FAISS Indices with CLIP Text Embeddings\n","\n","print(\"Creating CLIP text embeddings for all products...\")\n","print(\"This will take ~10 minutes for 44,417 products\")\n","\n","clip_text_embeddings = []\n","\n","batch_size = 100  # Batch processing\n","total_products = len(products_df)\n","\n","for i in range(0, total_products, batch_size):\n","    batch = products_df.iloc[i:i+batch_size]\n","\n","    for idx, row in batch.iterrows():\n","\n","        text = f\"{row['baseColour']} {row['articleType']}\"\n","\n","\n","        emb = encode_text_clip(text)\n","        clip_text_embeddings.append(emb)\n","\n","    # Progress\n","    if (i + batch_size) % 1000 == 0:\n","        print(f\"  Processed {i + batch_size}/{total_products} products...\")\n","\n","clip_text_embeddings = np.array(clip_text_embeddings).astype('float32')\n","print(f\"✓ CLIP text embeddings created: {clip_text_embeddings.shape}\")\n","\n","\n","\n","text_index = faiss.IndexFlatL2(clip_text_embeddings.shape[1])\n","text_index.add(clip_text_embeddings)\n","print(f'✓ Text index created with CLIP: {text_index.ntotal} vectors')\n","\n","\n","if image_embeddings is not None:\n","    image_index = faiss.IndexFlatL2(image_embeddings.shape[1])\n","    image_index.add(image_embeddings.astype('float32'))\n","    print(f'✓ Image index created: {image_index.ntotal} vectors')\n","else:\n","    image_index = None\n","    print('✗ Cannot create image index')\n","\n","\n","# Index → Product ID mapping\n","index_to_product_id = products_df['id'].values\n","print(f'✓ Created index-to-ID mapping: {len(index_to_product_id)} products')\n","\n","\n","# Initialize retriever\n","if text_index and image_index:\n","    retriever = MultiModalRetriever(\n","        text_index=text_index,\n","        image_index=image_index,\n","        products_df=products_df,\n","        alpha=0.7\n","    )\n","    print('✓ MultiModalRetriever initialized with CLIP text embeddings')\n","    print(f'  Alpha (text weight): {retriever.alpha}')\n","    print(f'  Text: CLIP, Image: CLIP → Consistent embedding space!')\n","else:\n","    print('✗ Cannot initialize retriever - missing embeddings')\n","    retriever = None"]},{"cell_type":"markdown","metadata":{"id":"MdYuR4epg81Z"},"source":["---\n","\n","## PART 6: Test Retrieval Strategies"]},{"cell_type":"code","execution_count":125,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6YMzDm6cg81Z","executionInfo":{"status":"ok","timestamp":1767908115054,"user_tz":-180,"elapsed":42601,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"2ad113a8-8af7-4c3e-8513-098334028664"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 5 test queries\n","\n","Testing: Mark Taylor Men White Striped Shirt\n","Query: 'white shirts'\n","  Text-only: 10 results\n","  Image-only: 10 results\n","  Multimodal: 10 results\n","\n","Testing: Arrow Woman Sylvia Blue Shirt\n","Query: 'blue shirts'\n","  Text-only: 10 results\n","  Image-only: 10 results\n","  Multimodal: 10 results\n","\n","Testing: Locomotive Men Check Ladislav Purple Shirts\n","Query: 'purple shirts'\n","  Text-only: 10 results\n","  Image-only: 10 results\n","  Multimodal: 10 results\n","\n","Testing: Highlander Men Solid Poplin Purple Shirts\n","Query: 'purple shirts'\n","  Text-only: 10 results\n","  Image-only: 10 results\n","  Multimodal: 10 results\n","\n","Testing: Jealous 21 Women Supper Zipped Ankle Black Jeans\n","Query: 'black jeans'\n","  Text-only: 10 results\n","  Image-only: 10 results\n","  Multimodal: 10 results\n","\n","Tested 5 queries\n"]}],"source":["# PART 6: Test Retrieval Strategies\n","\n","# Load test queries from Day 2\n","test_queries_df = pd.read_csv('v2.4.5-multimodal-rag/evaluation/results/image_queries.csv')\n","\n","print(f'Loaded {len(test_queries_df)} test queries')\n","\n","# Test all 3 strategies on sample queries\n","results_comparison = []\n","\n","for idx, row in test_queries_df.head(5).iterrows():\n","    product_id = row['product_id']\n","    product_name = row['original_name']\n","    text_query = row['generated_query']\n","\n","    # Get image path\n","    product_row = products_df[products_df['id'] == product_id].iloc[0]\n","    image_path = product_row['image_path']\n","\n","    # Fix path\n","    if 'ai_fashion_assistant_v1' in str(image_path):\n","        image_path = image_path.replace('ai_fashion_assistant_v1', 'ai_fashion_assistant_v2')\n","\n","    # Alternative path\n","    alt_path = f'data/raw/images/{product_id}.jpg'\n","\n","    # Check which path exists (with try-except)\n","    image_exists = False\n","    try:\n","        if Path(image_path).exists():\n","            image_exists = True\n","    except:\n","        pass\n","\n","    if not image_exists:\n","        try:\n","            if Path(alt_path).exists():\n","                image_path = alt_path\n","                image_exists = True\n","        except:\n","            pass\n","\n","    print(f\"\\nTesting: {product_name[:50]}\")\n","    print(f\"Query: '{text_query}'\")\n","\n","    # Strategy 1: Text-only\n","    text_results, _ = retriever.retrieve_by_text(text_query, k=10)\n","    print(f\"  Text-only: {len(text_results)} results\")\n","\n","    # Strategy 2: Image-only\n","    if image_exists:\n","        try:\n","            img_results, _ = retriever.retrieve_by_image(image_path, k=10)\n","            print(f\"  Image-only: {len(img_results)} results\")\n","        except Exception as e:\n","            img_results = []\n","            print(f\"  Image-only: Error - {str(e)[:50]}\")\n","    else:\n","        img_results = []\n","        print(f\"  Image-only: Image not available\")\n","\n","    # Strategy 3: Multimodal fusion\n","    if image_exists:\n","        try:\n","            mm_results = retriever.retrieve_multimodal(\n","                image_path=image_path,\n","                text_query=text_query,\n","                k=10\n","            )\n","            print(f\"  Multimodal: {len(mm_results)} results\")\n","        except Exception as e:\n","            mm_results = text_results[:10]\n","            print(f\"  Multimodal: Using text-only (error)\")\n","    else:\n","        mm_results = text_results[:10]\n","        print(f\"  Multimodal: Using text-only (no image)\")\n","\n","    # FAISS indices → Product IDs'e çevir\n","    text_product_ids = [index_to_product_id[idx] for idx in text_results]\n","    img_product_ids = [index_to_product_id[idx] for idx in img_results] if img_results else []\n","    mm_product_ids = [index_to_product_id[idx] for idx in mm_results]\n","\n","    results_comparison.append({\n","        'product_id': product_id,\n","        'product_name': product_name,\n","        'query': text_query,\n","        'text_only': text_product_ids,\n","        'image_only': img_product_ids,\n","        'multimodal': mm_product_ids\n","    })\n","\n","print(f'\\nTested {len(results_comparison)} queries')"]},{"cell_type":"markdown","metadata":{"id":"BFZJCUeTg81Z"},"source":["---\n","\n","## PART 7: Attribute-Based Filtering"]},{"cell_type":"code","execution_count":126,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vnI9Mv-ig81a","executionInfo":{"status":"ok","timestamp":1767908115789,"user_tz":-180,"elapsed":714,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"c957f467-5e0d-4596-fb2a-801fa9aa80e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded V2.1 attributes: 307720 rows\n","Transformed: (42388, 21)\n","Columns: ['product_id', 'fit', 'formality', 'length', 'material_appearance', 'neckline', 'occasion', 'pattern', 'season', 'sleeve']\n","\n","Sample:\n","category  product_id      pattern             style\n","0                  0    checkered  minimalist style\n","1                  1  solid color  minimalist style\n","2                  2          NaN  minimalist style\n","3                  3  solid color  minimalist style\n","4                  4  solid color  minimalist style\n","Filtering functions defined\n","\n","Mark Taylor Men White Striped Shirt:\n","  Before: 10, After: 10\n","  Avg score: 0.000\n","\n","Arrow Woman Sylvia Blue Shirt:\n","  Before: 10, After: 9\n","  Avg score: 0.722\n","\n","Locomotive Men Check Ladislav Purple Shi:\n","  Before: 10, After: 8\n","  Avg score: 0.812\n","\n","Highlander Men Solid Poplin Purple Shirt:\n","  Before: 10, After: 7\n","  Avg score: 0.857\n","\n","Jealous 21 Women Supper Zipped Ankle Bla:\n","  Before: 10, After: 9\n","  Avg score: 0.667\n","\n","✓ Filtering applied\n"]}],"source":["# PART 7: Attribute-Based Filtering - FIXED VERSION\n","\n","# V2.1 attributes yükle\n","v21_attrs_long = pd.read_csv('v2.1-core-ml-plus/evaluation/results/product_attributes.csv')\n","print(f\"Loaded V2.1 attributes: {v21_attrs_long.shape[0]} rows\")\n","\n","# Pivot - DOĞRU YÖNTEM\n","attr_df = v21_attrs_long.pivot_table(\n","    index='product_id',\n","    columns='category',\n","    values='value',\n","    aggfunc='first'\n",").reset_index()\n","\n","# Confidence'ları ayrı pivot\n","conf_df = v21_attrs_long.pivot_table(\n","    index='product_id',\n","    columns='category',\n","    values='confidence',\n","    aggfunc='first'\n",").reset_index()\n","\n","# Confidence column'larını rename\n","for col in conf_df.columns:\n","    if col != 'product_id':\n","        conf_df.rename(columns={col: f'{col}_confidence'}, inplace=True)\n","\n","# Merge\n","attr_df = attr_df.merge(conf_df, on='product_id')\n","\n","print(f\"Transformed: {attr_df.shape}\")\n","print(f\"Columns: {[c for c in attr_df.columns if 'confidence' not in c][:10]}\")\n","print(f\"\\nSample:\")\n","print(attr_df[['product_id', 'pattern', 'style']].head())\n","\n","\n","# Attribute matching - BASĐTLEŞTĐRĐLMĐŞ\n","def calculate_attribute_match(query_attrs: Dict, product_attrs: Dict) -> float:\n","    \"\"\"Calculate attribute matching score\"\"\"\n","    matches = 0\n","    total = 0\n","\n","    # Sadece pattern ve style karşılaştır (color v2.1'de yok olabilir)\n","    for attr_name in ['pattern', 'style']:\n","        if attr_name in query_attrs and attr_name in product_attrs:\n","            q_val = query_attrs.get(attr_name)\n","            p_val = product_attrs.get(attr_name)\n","\n","            if pd.notna(q_val) and pd.notna(p_val):\n","                total += 1\n","                # Substring match (daha esnek)\n","                if str(q_val).lower() in str(p_val).lower() or str(p_val).lower() in str(q_val).lower():\n","                    matches += 1\n","\n","    return matches / total if total > 0 else 0.0\n","\n","\n","# Filtering fonksiyonu - BASİTLEŞTİRİLMİŞ\n","def filter_by_attributes(results: List[int],\n","                        query_product_id: int,\n","                        attr_df: pd.DataFrame,\n","                        threshold: float = 0.2) -> List[Tuple[int, float]]:\n","    \"\"\"Filter by attributes\"\"\"\n","\n","    # Query product attributes\n","    query_row = attr_df[attr_df['product_id'] == query_product_id]\n","    if len(query_row) == 0:\n","        return [(pid, 0.0) for pid in results]\n","\n","    query_attrs = {\n","        'pattern': query_row.iloc[0].get('pattern'),\n","        'style': query_row.iloc[0].get('style')\n","    }\n","\n","    filtered = []\n","    for prod_id in results:\n","        prod_row = attr_df[attr_df['product_id'] == prod_id]\n","\n","        if len(prod_row) > 0:\n","            prod_attrs = {\n","                'pattern': prod_row.iloc[0].get('pattern'),\n","                'style': prod_row.iloc[0].get('style')\n","            }\n","            score = calculate_attribute_match(query_attrs, prod_attrs)\n","            if score >= threshold:\n","                filtered.append((prod_id, score))\n","        else:\n","            filtered.append((prod_id, 0.0))\n","\n","    filtered.sort(key=lambda x: x[1], reverse=True)\n","    return filtered\n","\n","\n","print('Filtering functions defined')\n","\n","\n","# Apply filtering\n","for result in results_comparison:\n","    product_id = result['product_id']\n","\n","    filtered = filter_by_attributes(\n","        result['multimodal'],\n","        product_id,\n","        attr_df,\n","        threshold=0.2\n","    )\n","\n","    result['filtered'] = [pid for pid, score in filtered]\n","    result['filter_scores'] = [score for pid, score in filtered]\n","\n","    print(f\"\\n{result['product_name'][:40]}:\")\n","    print(f\"  Before: {len(result['multimodal'])}, After: {len(result['filtered'])}\")\n","    if result['filter_scores']:\n","        print(f\"  Avg score: {np.mean(result['filter_scores']):.3f}\")\n","\n","print('\\n✓ Filtering applied')"]},{"cell_type":"markdown","metadata":{"id":"iYhQQTsxg81a"},"source":["---\n","\n","## PART 8: Results Analysis"]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IE6m_XISg81a","executionInfo":{"status":"ok","timestamp":1767908115806,"user_tz":-180,"elapsed":7,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"39a5fd4f-0a23-4aa9-9d50-c41a9a2e3609"},"outputs":[{"output_type":"stream","name":"stdout","text":["Retrieval Strategy Overlap Analysis\n","============================================================\n","                            product_name  text_only_count  image_only_count  multimodal_count  text_image_overlap  multimodal_unique\n","     Mark Taylor Men White Striped Shirt               10                10                10                   0                  7\n","           Arrow Woman Sylvia Blue Shirt               10                10                10                   0                  7\n","Locomotive Men Check Ladislav Purple Shi               10                10                10                   0                  5\n","Highlander Men Solid Poplin Purple Shirt               10                10                10                   0                  6\n","Jealous 21 Women Supper Zipped Ankle Bla               10                10                10                   2                  5\n","\n","Average Overlap:\n","  Text-Image: 0.4 products\n","  Multimodal unique: 6.0 products\n"]}],"source":["# Analyze overlap between strategies\n","overlap_stats = []\n","\n","for result in results_comparison:\n","    text_set = set(result['text_only'])\n","    img_set = set(result['image_only']) if result['image_only'] else set()\n","    mm_set = set(result['multimodal'])\n","\n","    overlap_stats.append({\n","        'product_name': result['product_name'][:40],\n","        'text_only_count': len(text_set),\n","        'image_only_count': len(img_set),\n","        'multimodal_count': len(mm_set),\n","        'text_image_overlap': len(text_set & img_set) if img_set else 0,\n","        'multimodal_unique': len(mm_set - text_set - img_set)\n","    })\n","\n","overlap_df = pd.DataFrame(overlap_stats)\n","\n","print('Retrieval Strategy Overlap Analysis')\n","print('='*60)\n","print(overlap_df.to_string(index=False))\n","print('\\nAverage Overlap:')\n","print(f\"  Text-Image: {overlap_df['text_image_overlap'].mean():.1f} products\")\n","print(f\"  Multimodal unique: {overlap_df['multimodal_unique'].mean():.1f} products\")"]},{"cell_type":"code","source":["# OVERLAP SORUNUNU ANALİZ ET\n","\n","print(\"=== DETAILED OVERLAP ANALYSIS ===\\n\")\n","\n","for i, result in enumerate(results_comparison):\n","    print(f\"\\n{i+1}. {result['product_name'][:50]}\")\n","    print(f\"   Query Product ID: {result['product_id']}\")\n","\n","    text_set = set(result['text_only'])\n","    img_set = set(result['image_only'])\n","    mm_set = set(result['multimodal'])\n","\n","    # Intersection\n","    text_img_overlap = text_set & img_set\n","\n","    print(f\"\\n   Text-only top 5: {result['text_only'][:5]}\")\n","    print(f\"   Image-only top 5: {result['image_only'][:5]}\")\n","    print(f\"   Multimodal top 5: {result['multimodal'][:5]}\")\n","\n","    print(f\"\\n   Text ∩ Image: {len(text_img_overlap)} products\")\n","    if text_img_overlap:\n","        print(f\"   Overlapping IDs: {list(text_img_overlap)[:5]}\")\n","\n","    # Check product names\n","    print(f\"\\n   Text results (names):\")\n","    for pid in result['text_only'][:3]:\n","        prod = products_df[products_df['id'] == pid]\n","        if len(prod) > 0:\n","            print(f\"     {pid}: {prod.iloc[0]['productDisplayName'][:50]}\")\n","\n","    print(f\"\\n   Image results (names):\")\n","    for pid in result['image_only'][:3]:\n","        prod = products_df[products_df['id'] == pid]\n","        if len(prod) > 0:\n","            print(f\"     {pid}: {prod.iloc[0]['productDisplayName'][:50]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hP-YLwxENCdb","executionInfo":{"status":"ok","timestamp":1767908115823,"user_tz":-180,"elapsed":15,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"9339e160-de42-46df-df38-e25944d71f7e"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["=== DETAILED OVERLAP ANALYSIS ===\n","\n","\n","1. Mark Taylor Men White Striped Shirt\n","   Query Product ID: 8859\n","\n","   Text-only top 5: [np.int64(4943), np.int64(59297), np.int64(4944), np.int64(14052), np.int64(9463)]\n","   Image-only top 5: [np.int64(8859), np.int64(8861), np.int64(8853), np.int64(8872), np.int64(8866)]\n","   Multimodal top 5: [np.int64(8723), np.int64(16358), np.int64(43304), np.int64(10803), np.int64(9464)]\n","\n","   Text ∩ Image: 0 products\n","\n","   Text results (names):\n","     4943: Gini and Jony Boy's Kaleb White Brown Kidswear\n","     59297: U.S. Polo Assn. Men White & Navy Blue Shirt\n","     4944: Gini and Jony Boy's Kaleb White Brown Infant Kidsw\n","\n","   Image results (names):\n","     8859: Mark Taylor Men White Striped Shirt\n","     8861: Mark Taylor Men White Striped Shirt\n","     8853: Mark Taylor Men White Striped Shirt\n","\n","2. Arrow Woman Sylvia Blue Shirt\n","   Query Product ID: 18889\n","\n","   Text-only top 5: [np.int64(22395), np.int64(12190), np.int64(34036), np.int64(13846), np.int64(22359)]\n","   Image-only top 5: [np.int64(18889), np.int64(18763), np.int64(18759), np.int64(18762), np.int64(18885)]\n","   Multimodal top 5: [np.int64(12190), np.int64(26795), np.int64(9235), np.int64(24267), np.int64(14037)]\n","\n","   Text ∩ Image: 0 products\n","\n","   Text results (names):\n","     22395: Mark Taylor Men Striped Blue Shirt\n","     12190: Basics Men Blue Slim Fit Checked Shirt\n","     34036: Palm Tree Boys Check Blue Shirt\n","\n","   Image results (names):\n","     18889: Arrow Woman Sylvia Blue Shirt\n","     18763: Arrow Woman Ariella White Shirt\n","     18759: Arrow Woman Aiyanna Blue Shirt\n","\n","3. Locomotive Men Check Ladislav Purple Shirts\n","   Query Product ID: 8181\n","\n","   Text-only top 5: [np.int64(26960), np.int64(12369), np.int64(7167), np.int64(9667), np.int64(37224)]\n","   Image-only top 5: [np.int64(8181), np.int64(8151), np.int64(8182), np.int64(8157), np.int64(8186)]\n","   Multimodal top 5: [np.int64(8181), np.int64(13760), np.int64(8188), np.int64(6618), np.int64(11695)]\n","\n","   Text ∩ Image: 0 products\n","\n","   Text results (names):\n","     26960: Jealous 21 Women Purple Shirt\n","     12369: Reid & Taylor Men Check Purple Shirts\n","     7167: Scullers Men Scul Purple White Shirt\n","\n","   Image results (names):\n","     8181: Locomotive Men Check Ladislav Purple Shirts\n","     8151: Highlander Men Purple Checked Hadwin Slim Fit Shir\n","     8182: Locomotive Men Check Ladomar White Shirts\n","\n","4. Highlander Men Solid Poplin Purple Shirts\n","   Query Product ID: 5960\n","\n","   Text-only top 5: [np.int64(26960), np.int64(12369), np.int64(7167), np.int64(9667), np.int64(37224)]\n","   Image-only top 5: [np.int64(5960), np.int64(6004), np.int64(6981), np.int64(5953), np.int64(26415)]\n","   Multimodal top 5: [np.int64(6618), np.int64(18321), np.int64(13395), np.int64(26960), np.int64(12369)]\n","\n","   Text ∩ Image: 0 products\n","\n","   Text results (names):\n","     26960: Jealous 21 Women Purple Shirt\n","     12369: Reid & Taylor Men Check Purple Shirts\n","     7167: Scullers Men Scul Purple White Shirt\n","\n","   Image results (names):\n","     5960: Highlander Men Solid Poplin Purple Shirts\n","     6004: Highlander Men Blue Bold Green Checked Shirt\n","     6981: s.Oliver Men Purple Black Check Shirt\n","\n","5. Jealous 21 Women Supper Zipped Ankle Black Jeans\n","   Query Product ID: 7187\n","\n","   Text-only top 5: [np.int64(26994), np.int64(7193), np.int64(7709), np.int64(39381), np.int64(11520)]\n","   Image-only top 5: [np.int64(7187), np.int64(7190), np.int64(7189), np.int64(9145), np.int64(7707)]\n","   Multimodal top 5: [np.int64(7187), np.int64(7707), np.int64(9142), np.int64(7193), np.int64(7709)]\n","\n","   Text ∩ Image: 2 products\n","   Overlapping IDs: [np.int64(7193), np.int64(7707)]\n","\n","   Text results (names):\n","     26994: Jealous 21 Women Black Jeans\n","     7193: Jealous 21 Women Black Jegging\n","     7709: Jealous 21 Women's Aaren Black Jegging\n","\n","   Image results (names):\n","     7187: Jealous 21 Women Supper Zipped Ankle Black Jeans\n","     7190: Jealous 21 Women Rinse Wash Black Jegging\n","     7189: Jealous 21 Women Rinse Wash Blue Jegging\n"]}]},{"cell_type":"markdown","metadata":{"id":"UTXNKL6Zg81a"},"source":["---\n","\n","## PART 9: Save Results"]},{"cell_type":"code","execution_count":130,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKnCplZMg81a","executionInfo":{"status":"ok","timestamp":1767908645144,"user_tz":-180,"elapsed":173,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"0f30e7ac-b0af-41e6-d765-e3eab37cf5b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: v2.4.5-multimodal-rag/evaluation/results/retrieval_comparison.json\n","Saved: v2.4.5-multimodal-rag/evaluation/results/strategy_overlap.csv\n","Saved: v2.4.5-multimodal-rag/evaluation/results/attribute_filtering_results.csv\n","\n","All results saved\n"]}],"source":["# Save retrieval results\n","EVAL_DIR = Path('v2.4.5-multimodal-rag/evaluation/results')\n","EVAL_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Save comparison results (JSON)\n","with open(EVAL_DIR / 'retrieval_comparison.json', 'w') as f:\n","    json.dump(results_comparison, f, indent=2, default=str)\n","print(f'Saved: {EVAL_DIR / \"retrieval_comparison.json\"}')\n","\n","# Save overlap analysis (CSV)\n","overlap_df.to_csv(EVAL_DIR / 'strategy_overlap.csv', index=False)\n","print(f'Saved: {EVAL_DIR / \"strategy_overlap.csv\"}')\n","\n","# Save filtered results\n","filtered_results = []\n","for result in results_comparison:\n","    if 'filtered' in result:\n","        filtered_results.append({\n","            'product_id': result['product_id'],\n","            'product_name': result['product_name'],\n","            'filtered_count': len(result['filtered']),\n","            'avg_match_score': np.mean(result['filter_scores']) if result['filter_scores'] else 0\n","        })\n","\n","filtered_df = pd.DataFrame(filtered_results)\n","filtered_df.to_csv(EVAL_DIR / 'attribute_filtering_results.csv', index=False)\n","print(f'Saved: {EVAL_DIR / \"attribute_filtering_results.csv\"}')\n","\n","print('\\nAll results saved')"]},{"cell_type":"markdown","metadata":{"id":"jga_ATheg81a"},"source":["---\n","\n","## Summary"]},{"cell_type":"code","execution_count":131,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OvOra8_g81a","executionInfo":{"status":"ok","timestamp":1767908647224,"user_tz":-180,"elapsed":49,"user":{"displayName":"Hatice Baydemir","userId":"09255724962739063380"}},"outputId":"97b6fa7c-5e36-4881-d196-3b2eedbde7df"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","DAY 3: MULTIMODAL RETRIEVAL COMPLETE\n","============================================================\n","\n","Completed:\n","  ✓ FAISS indices loaded (text + image)\n","  ✓ MultiModalRetriever class implemented\n","  ✓ 3 retrieval strategies tested\n","  ✓ Fusion with α=0.7 (70% text, 30% image)\n","  ✓ Attribute-based filtering\n","  ✓ 5 queries compared\n","\n","Key Findings:\n","  - Text-Image overlap: 0.4 products\n","  - Multimodal adds: 6.0 unique products\n","  - Attribute filtering: 0.612 avg match\n","\n","Output Files:\n","  - retrieval_comparison.json\n","  - strategy_overlap.csv\n","  - attribute_filtering_results.csv\n","\n","Next Steps (Day 4):\n","  1. Load v2.2 RAG pipeline\n","  2. Create VisualRAGPipeline class\n","  3. Generate visual-aware prompts\n","  4. Test with multimodal retrieval\n","  5. Response quality check\n","============================================================\n"]}],"source":["print('='*60)\n","print('DAY 3: MULTIMODAL RETRIEVAL COMPLETE')\n","print('='*60)\n","\n","print('\\nCompleted:')\n","print(f'  ✓ FAISS indices loaded (text + image)')\n","print(f'  ✓ MultiModalRetriever class implemented')\n","print(f'  ✓ 3 retrieval strategies tested')\n","print(f'  ✓ Fusion with α={retriever.alpha} (70% text, 30% image)')\n","print(f'  ✓ Attribute-based filtering')\n","print(f'  ✓ {len(results_comparison)} queries compared')\n","\n","print('\\nKey Findings:')\n","print(f'  - Text-Image overlap: {overlap_df[\"text_image_overlap\"].mean():.1f} products')\n","print(f'  - Multimodal adds: {overlap_df[\"multimodal_unique\"].mean():.1f} unique products')\n","print(f'  - Attribute filtering: {filtered_df[\"avg_match_score\"].mean():.3f} avg match')\n","\n","print('\\nOutput Files:')\n","print('  - retrieval_comparison.json')\n","print('  - strategy_overlap.csv')\n","print('  - attribute_filtering_results.csv')\n","\n","print('\\nNext Steps (Day 4):')\n","print('  1. Load v2.2 RAG pipeline')\n","print('  2. Create VisualRAGPipeline class')\n","print('  3. Generate visual-aware prompts')\n","print('  4. Test with multimodal retrieval')\n","print('  5. Response quality check')\n","\n","print('='*60)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}