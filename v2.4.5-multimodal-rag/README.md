# AI Fashion Assistant v2.4.5 - Multimodal RAG

**TÃœBÄ°TAK 2209-A Undergraduate Research Project**  
**Student:** Hatice Baydemir  
**Advisor:** Ä°lya KuÅŸ  
**Institution:** KaramanoÄŸlu Mehmetbey University  

---

## ğŸ¯ Project Overview

AI Fashion Assistant v2.4.5 is an advanced multimodal fashion search and recommendation system that combines:
- **Text search** using semantic embeddings
- **Image search** using CLIP visual features
- **Multimodal fusion** with learned weights
- **Visual-aware RAG** for intelligent responses

### Key Features

âœ… **Image Query Support** - Search using product images  
âœ… **Multimodal Fusion** - Combines text and visual signals (Î±=0.7)  
âœ… **Visual Awareness** - 7.6 visual keywords per response  
âœ… **Fast Response** - 0.64s average (28% faster than v2.2)  
âœ… **Comprehensive Coverage** - 44,417 products indexed  

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **Dataset Size** | 44,417 products |
| **Multimodal Unique** | 6.0 products avg |
| **Text-Image Overlap** | 0.4 products avg |
| **Response Time** | 0.642s avg |
| **Visual Keywords** | 7.6 per response |
| **Attribute Coverage** | 95.4% (42,388 products) |

---

## ğŸ—ï¸ System Architecture

```
User Query (Text/Image)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query Processing             â”‚
â”‚  - CLIP Text Encoding (768d)  â”‚
â”‚  - CLIP Image Encoding (768d) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multimodal Retrieval         â”‚
â”‚  - Text FAISS Index           â”‚
â”‚  - Image FAISS Index          â”‚
â”‚  - Learned Fusion (Î±=0.7)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Attribute Filtering          â”‚
â”‚  - V2.1 Visual Attributes     â”‚
â”‚  - Pattern/Style Matching     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Visual-Aware RAG             â”‚
â”‚  - Context Building           â”‚
â”‚  - Visual Attribute Prompts   â”‚
â”‚  - GROQ Llama-3.3-70B         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   Response to User
```

---

## ğŸ“ Repository Structure

```
ai_fashion_assistant_v2/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â””â”€â”€ meta_ssot.csv              # 44,417 products
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ images/                     # Product images
â”œâ”€â”€ v2.0-baseline/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ text/mpnet_768d.npy        # MPNet embeddings
â”‚   â”‚   â””â”€â”€ image/clip_image_768d.npy  # CLIP image embeddings
â”‚   â””â”€â”€ notebooks/                      # 30+ notebooks
â”œâ”€â”€ v2.1-core-ml-plus/
â”‚   â””â”€â”€ evaluation/results/
â”‚       â””â”€â”€ product_attributes.csv      # 307K visual attributes
â”œâ”€â”€ v2.2-rag/
â”‚   â””â”€â”€ notebooks/                      # RAG implementation
â”œâ”€â”€ v2.4-personalization/
â”‚   â””â”€â”€ notebooks/                      # User personalization
â”œâ”€â”€ v2.4.5-multimodal-rag/
â”‚   â”œâ”€â”€ notebooks/
â”‚   â”‚   â”œâ”€â”€ 01_multimodal_rag_architecture.ipynb
â”‚   â”‚   â”œâ”€â”€ 02_image_query_processing.ipynb
â”‚   â”‚   â”œâ”€â”€ 03_multimodal_retrieval.ipynb
â”‚   â”‚   â”œâ”€â”€ 04_visual_aware_rag.ipynb
â”‚   â”‚   â”œâ”€â”€ 05_evaluation_metrics.ipynb
â”‚   â”‚   â””â”€â”€ 06_final_documentation.ipynb
â”‚   â””â”€â”€ evaluation/results/
â”‚       â”œâ”€â”€ retrieval_comparison.json
â”‚       â”œâ”€â”€ visual_rag_responses.json
â”‚       â”œâ”€â”€ performance_report.md
â”‚       â”œâ”€â”€ performance_visualization.png
â”‚       â””â”€â”€ v2.4.5_comprehensive_results.xlsx
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Install dependencies
pip install numpy pandas torch transformers
pip install faiss-cpu sentence-transformers
pip install groq pillow opencv-python
```

### 2. Load Models

```python
from transformers import CLIPModel, CLIPProcessor

# Load CLIP
model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
```

### 3. Run Multimodal Search

```python
# Text query
results_text = retriever.retrieve_by_text("white shirts", k=10)

# Image query
results_image = retriever.retrieve_by_image("path/to/image.jpg", k=10)

# Multimodal fusion
results_multimodal = retriever.retrieve_multimodal(
    text_query="white shirts",
    image_path="path/to/image.jpg",
    k=10
)
```

---

## ğŸ“ˆ Version History

### v2.0 - Text-Only Baseline (Completed)
- MPNet text embeddings
- FAISS HNSW indexing
- NDCG@10: 0.974

### v2.1 - Core ML+ (Completed)
- CLIP visual features
- 307K visual attributes extracted
- Learned fusion weights

### v2.2 - RAG System (Completed)
- GROQ Llama-3.3-70B
- Context-aware responses
- RAG Score: 0.714

### v2.4 - Personalization (Completed)
- User profile management
- Content-based filtering
- 76.7% preference match

### v2.4.5 - Multimodal RAG (Current)
- Image query support
- Multimodal fusion retrieval
- Visual-aware RAG responses
- 7.6 visual keywords per response

---

## ğŸ“Š Evaluation Results

### Retrieval Performance
- Text-only: 10 results per query
- Image-only: 10 results per query
- Multimodal: 10 fused results
- Unique products via fusion: 6.0 avg

### RAG Quality
- Response time: 0.642s avg (0.581s - 0.727s)
- Response length: ~496 characters
- Visual awareness: 7.6 keywords per response
- Improvement vs v2.2: 28% faster, 100% more visual

---

## ğŸ“ Academic Contribution

### Novel Aspects
1. **Multimodal Fashion Search** - First implementation combining CLIP text/image for Turkish fashion dataset
2. **Learned Fusion Strategy** - Empirically derived Î±=0.7 weight
3. **Visual-Aware RAG** - Integration of visual attributes in LLM prompts
4. **Production-Ready System** - Sub-second response time at scale

### Technical Innovations
- CLIP text embeddings for all 44K products (vs MPNet baseline)
- V2.1 attribute integration (307K visual features)
- Attribute-based post-filtering
- Visual reasoning in natural language responses

---

## ğŸ‘¥ Team

**Student Researcher:** Hatice Baydemir  
**Advisor:** Ä°lya KuÅŸ  
**Institution:** KaramanoÄŸlu Mehmetbey University  
**Department:** Computer Engineering  
**Program:** TÃœBÄ°TAK 2209-A Undergraduate Research  

---

**Last Updated:** January 2026  
**Version:** 2.4.5  
**Status:** âœ… Complete - Ready for User Study
