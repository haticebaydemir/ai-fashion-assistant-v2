# User Study Guide - v2.4.5 Multimodal RAG

**Study Period:** Week 2-3 (January 13-24, 2026)  
**Participants:** 20-25 volunteers  
**Duration:** ~30 minutes per session

---

## Study Design

### Objective
Compare user satisfaction across three versions:
1. **v2.0** - Text-only baseline
2. **v2.4** - Personalized system
3. **v2.4.5** - Multimodal RAG

### Methodology
- **Design:** Within-subjects (each participant tests all 3 versions)
- **Order:** Randomized to control for learning effects
- **Tasks:** 5 search scenarios per version
- **Metrics:** SUS score, preference rating, task completion time

---

## Test Scenarios

### Scenario 1: Text Query
**Task:** "Find a white formal shirt for office wear"
- Evaluate: Relevance, diversity, visual appeal

### Scenario 2: Image Query (v2.4.5 only)
**Task:** Upload a product image and find similar items
- Evaluate: Visual similarity, alternative suggestions

### Scenario 3: Conversational Search
**Task:** "I need something for a summer wedding"
- Evaluate: Response quality, product recommendations

### Scenario 4: Refinement
**Task:** "Show me the same but in blue"
- Evaluate: Understanding, adaptation

### Scenario 5: Complex Query
**Task:** "Casual jeans for weekend, not too formal"
- Evaluate: Nuance understanding, filtering

---

## Questionnaire

### System Usability Scale (SUS)
1. I think I would like to use this system frequently (1-5)
2. I found the system unnecessarily complex (1-5)
3. I thought the system was easy to use (1-5)
4. I would need technical support to use this system (1-5)
5. The system's features were well integrated (1-5)
6. There was too much inconsistency in the system (1-5)
7. Most people would learn to use this system quickly (1-5)
8. I found the system very cumbersome to use (1-5)
9. I felt very confident using the system (1-5)
10. I needed to learn a lot before using this system (1-5)

### Custom Questions
11. The search results matched what I was looking for (1-5)
12. The response time was acceptable (1-5)
13. The system understood my visual preferences (1-5)
14. The recommendations were diverse (1-5)
15. I trust the system's suggestions (1-5)

### Open-ended
16. What did you like most about this version?
17. What would you improve?
18. Which version do you prefer overall? Why?

---

## Data Collection

### Quantitative Metrics
- SUS score (0-100)
- Task completion time
- Number of query refinements
- Click-through rate
- Preference ranking

### Qualitative Data
- Think-aloud observations
- Open-ended feedback
- Pain points identified
- Feature requests

---

## Analysis Plan

### Statistical Tests
- Repeated measures ANOVA (SUS scores across versions)
- Friedman test (preference rankings)
- Pairwise t-tests with Bonferroni correction

### Visualization
- Box plots: SUS scores by version
- Bar charts: Preference distribution
- Heatmap: Task completion times

---

## Expected Outcomes

### Hypotheses
- H1: v2.4.5 will have higher SUS scores than v2.0
- H2: v2.4.5 will be preferred for visual search tasks
- H3: Response quality ratings will be highest for v2.4.5

### Success Criteria
- SUS score > 70 (above average)
- >60% prefer v2.4.5 overall
- Task completion time < 2 minutes

---

## Recruitment

### Target Participants
- Age: 18-45
- Demographics: Balanced gender
- Experience: Regular online shoppers
- Tech savvy: Mixed levels

### Exclusion Criteria
- No prior knowledge of the project
- Not affiliated with research team

---

## Ethics

- Informed consent required
- Anonymous data collection
- Right to withdraw anytime
- No compensation (volunteer basis)

---

**Prepared by:** Hatice Baydemir  
**Approved by:** İlya Kuş  
**Date:** January 2026
