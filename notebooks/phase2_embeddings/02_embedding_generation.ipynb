{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njZPYnzbqO-w"
   },
   "source": [
    "# üöÄ AI Fashion Assistant v2.0 - Embedding Generation\n",
    "\n",
    "**Phase 2, Notebook 2/3** - üî• **GPU INTENSIVE!**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "1. Load all 44,418 products\n",
    "2. Generate **text embeddings** (mpnet + CLIP text)\n",
    "3. Generate **image embeddings** (CLIP image)\n",
    "4. Save embeddings to disk\n",
    "5. Validate dimensions and quality\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è GPU REQUIREMENTS\n",
    "\n",
    "- **GPU:** A100 (40GB) or V100 (16GB)\n",
    "- **Time:** 2-3 hours\n",
    "- **Disk:** ~3 GB for embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Expected Outputs\n",
    "\n",
    "```\n",
    "embeddings/\n",
    "‚îú‚îÄ‚îÄ text/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ mpnet_768d.npy          (44,418 x 768)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ clip_text_512d.npy      (44,418 x 512)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ combined_1280d.npy      (44,418 x 1280)\n",
    "‚îî‚îÄ‚îÄ image/\n",
    "    ‚îî‚îÄ‚îÄ clip_image_768d.npy     (44,418 x 768)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Quality Gates\n",
    "\n",
    "- ‚úì All embeddings generated (no NaNs)\n",
    "- ‚úì Dimensions correct\n",
    "- ‚úì File sizes reasonable (~2-3 GB total)\n",
    "- ‚úì Embeddings normalized\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1766134900761,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "hW2gYWdcqO-y",
    "outputId": "2141229b-72e8-4d5d-a362-433bf43b49f7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) SETUP & GPU CHECK\n",
    "# ============================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=False)\n",
    "\n",
    "# Check GPU\n",
    "print(\"üîç GPU CHECK\")\n",
    "print(\"=\" * 80)\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n‚ùå WARNING: GPU not available!\")\n",
    "    print(\"   This notebook requires GPU. Please enable GPU runtime.\")\n",
    "    raise RuntimeError(\"GPU required!\")\n",
    "\n",
    "print(f\"\\n‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17979,
     "status": "ok",
     "timestamp": 1766134918744,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "OTQDgtnDqO-z",
    "outputId": "4fa92909-f2b5-49fe-f2fc-fd7e1188011c"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) INSTALL/UPGRADE PACKAGES\n",
    "# ============================================================\n",
    "\n",
    "print(\"üì¶ Installing packages...\\n\")\n",
    "\n",
    "!pip install -q --upgrade sentence-transformers\n",
    "!pip install -q --upgrade transformers\n",
    "!pip install -q --upgrade torch torchvision\n",
    "!pip install -q pillow tqdm\n",
    "\n",
    "print(\"\\n‚úÖ Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1766134918801,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "lI_VkwkEqO-0",
    "outputId": "cac2d026-723b-40f2-ecd6-67db00034cb9"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Transformers (for CLIP)\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1766134919078,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "mULf0CxpqO-0",
    "outputId": "7f3d4f82-9ac4-4724-ebea-b2d853f851ef"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) PATHS & CONFIG\n",
    "# ============================================================\n",
    "\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/ai_fashion_assistant_v2\")\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data/processed\"\n",
    "EMB_DIR = PROJECT_ROOT / \"embeddings\"\n",
    "EMB_TEXT_DIR = EMB_DIR / \"text\"\n",
    "EMB_IMAGE_DIR = EMB_DIR / \"image\"\n",
    "\n",
    "# Create directories\n",
    "EMB_TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMB_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load config\n",
    "config_path = PROJECT_ROOT / \"embeddings/configs/model_config.json\"\n",
    "with open(config_path, 'r') as f:\n",
    "    MODEL_CONFIG = json.load(f)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_CONFIG['device'] = device\n",
    "\n",
    "print(\"üìã Configuration:\")\n",
    "print(\"=\" * 80)\n",
    "for key, value in MODEL_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Batch sizes (adjust based on GPU memory)\n",
    "TEXT_BATCH_SIZE = 256\n",
    "IMAGE_BATCH_SIZE = 64\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Batch sizes:\")\n",
    "print(f\"  Text: {TEXT_BATCH_SIZE}\")\n",
    "print(f\"  Image: {IMAGE_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1766134919338,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "E1PyTNdJqO-1",
    "outputId": "a7d1ee67-071a-4bdc-80da-a9a45f064f4e"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) LOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìÇ Loading product data...\\n\")\n",
    "\n",
    "# Load SSOT data\n",
    "df = pd.read_csv(PROCESSED_DIR / \"meta_ssot.csv\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df):,} products\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Check required fields\n",
    "required_fields = ['id', 'desc', 'image_path']\n",
    "missing_fields = [f for f in required_fields if f not in df.columns]\n",
    "\n",
    "if missing_fields:\n",
    "    raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All required fields present\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample products:\")\n",
    "display(df[['id', 'productDisplayName', 'desc']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4831,
     "status": "ok",
     "timestamp": 1766134924172,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "GMGsCJQcqO-2",
    "outputId": "ca538b2b-4eac-4fed-9abf-bfccd74d53f7"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6) LOAD MODELS\n",
    "# ============================================================\n",
    "\n",
    "print(\"ü§ñ LOADING MODELS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Primary text model (mpnet)\n",
    "print(\"\\n1Ô∏è‚É£ Loading mpnet...\")\n",
    "start_time = time.time()\n",
    "text_model_primary = SentenceTransformer(MODEL_CONFIG[\"text_model_primary\"])\n",
    "text_model_primary = text_model_primary.to(device)\n",
    "print(f\"   ‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "# CLIP model\n",
    "print(\"\\n2Ô∏è‚É£ Loading CLIP...\")\n",
    "start_time = time.time()\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_CONFIG[\"image_model\"])\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_CONFIG[\"image_model\"])\n",
    "clip_model = clip_model.to(device)\n",
    "print(f\"   ‚úÖ Loaded in {time.time() - start_time:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ All models loaded!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "e8fda76b0cf74a3eb3aa0098e8fb6f68",
      "f2d86d16849f41d8893ac7d566bc49ba",
      "cd5c2147d3b045729940d650459e7d7f",
      "bd070ccb3b7346f29dc7a8a638df2416",
      "2887210e3e404be2b3902e8a31bcbe90",
      "be135ced66ff49c7877c442a2afb863a",
      "dfd2929d2da34b5abf98c32d1211aa7a",
      "34294ffff63445c58126878d8b0ea0bc",
      "f5e97c48641d47b99a68d5d0da8c5b8d",
      "ef7a15dce216483ca478724c0cc1877d",
      "4c94eb9d3401404b98cb617e217da575"
     ]
    },
    "executionInfo": {
     "elapsed": 21033,
     "status": "ok",
     "timestamp": 1766134945246,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "1ObZkm9TqO-3",
    "outputId": "dd9551e0-817f-4b9a-c922-5d259eec4c9d"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7) GENERATE TEXT EMBEDDINGS (MPNET)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìù GENERATING TEXT EMBEDDINGS (MPNET)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare texts\n",
    "texts = df['desc'].fillna('').tolist()\n",
    "print(f\"Total texts: {len(texts):,}\")\n",
    "print(f\"Batch size: {TEXT_BATCH_SIZE}\")\n",
    "print(f\"Estimated time: {len(texts) / TEXT_BATCH_SIZE * 2 / 60:.1f} minutes\\n\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "mpnet_embeddings = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), TEXT_BATCH_SIZE), desc=\"mpnet batches\"):\n",
    "    batch = texts[i:i+TEXT_BATCH_SIZE]\n",
    "\n",
    "    # Encode\n",
    "    batch_embs = text_model_primary.encode(\n",
    "        batch,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=False,\n",
    "        batch_size=TEXT_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    mpnet_embeddings.append(batch_embs)\n",
    "\n",
    "# Concatenate\n",
    "mpnet_embeddings = np.vstack(mpnet_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Generated mpnet embeddings\")\n",
    "print(f\"   Shape: {mpnet_embeddings.shape}\")\n",
    "print(f\"   Expected: ({len(texts)}, {MODEL_CONFIG['text_model_primary_dim']})\")\n",
    "print(f\"   Time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"   Speed: {elapsed / len(texts) * 1000:.1f}ms per text\")\n",
    "\n",
    "# Validate\n",
    "assert mpnet_embeddings.shape == (len(texts), MODEL_CONFIG['text_model_primary_dim'])\n",
    "assert not np.isnan(mpnet_embeddings).any(), \"NaN values detected!\"\n",
    "\n",
    "# Save\n",
    "output_path = EMB_TEXT_DIR / \"mpnet_768d.npy\"\n",
    "np.save(output_path, mpnet_embeddings)\n",
    "print(f\"\\nüíæ Saved: {output_path}\")\n",
    "print(f\"   Size: {output_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327,
     "referenced_widgets": [
      "a1ee1d250d414d77875713b86c9c6e11",
      "f470066070504b339d4c79c1b103bfcc",
      "8b92a928493d4239aa9b249517072a86",
      "eb5ee794f0a446bca6ef176228456109",
      "e6c4b7a584214823b95e0403ff22eebd",
      "be45c03dc81348b1abfaed0a9a74e341",
      "7cdb19d75fb54fd08766f7c77d7a681a",
      "2b0c4d51d5754119b5712dea7eeee865",
      "6187634d46e24c2186fbeada36de7dd0",
      "e267f18a64bd4367bff22a702874d08b",
      "3a4818e3975c423fbf54f510a233e6c8"
     ]
    },
    "executionInfo": {
     "elapsed": 18419,
     "status": "ok",
     "timestamp": 1766134963703,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "dZ4xlGVOqO-4",
    "outputId": "9e0d003f-262e-43f1-f210-be95fb862cbc"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 8) GENERATE TEXT EMBEDDINGS (CLIP TEXT)\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìù GENERATING TEXT EMBEDDINGS (CLIP TEXT)...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Total texts: {len(texts):,}\")\n",
    "print(f\"Batch size: {TEXT_BATCH_SIZE}\")\n",
    "print(f\"Estimated time: {len(texts) / TEXT_BATCH_SIZE * 3 / 60:.1f} minutes\\n\")\n",
    "\n",
    "# Generate embeddings in batches\n",
    "clip_text_embeddings = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(texts), TEXT_BATCH_SIZE), desc=\"CLIP text batches\"):\n",
    "    batch = texts[i:i+TEXT_BATCH_SIZE]\n",
    "\n",
    "    # Process\n",
    "    inputs = clip_processor(text=batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Encode\n",
    "    with torch.no_grad():\n",
    "        batch_embs = clip_model.get_text_features(**inputs)\n",
    "        batch_embs = batch_embs.cpu().numpy()\n",
    "\n",
    "    clip_text_embeddings.append(batch_embs)\n",
    "\n",
    "    # Clear GPU cache every 10 batches\n",
    "    if i % (TEXT_BATCH_SIZE * 10) == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate\n",
    "clip_text_embeddings = np.vstack(clip_text_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Generated CLIP text embeddings\")\n",
    "print(f\"   Shape: {clip_text_embeddings.shape}\")\n",
    "print(f\"   Expected: ({len(texts)}, {MODEL_CONFIG['text_model_secondary_dim']})\")\n",
    "print(f\"   Time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"   Speed: {elapsed / len(texts) * 1000:.1f}ms per text\")\n",
    "\n",
    "# Validate\n",
    "assert clip_text_embeddings.shape == (len(texts), MODEL_CONFIG['text_model_secondary_dim'])\n",
    "assert not np.isnan(clip_text_embeddings).any(), \"NaN values detected!\"\n",
    "\n",
    "# Save\n",
    "output_path = EMB_TEXT_DIR / \"clip_text_512d.npy\"\n",
    "np.save(output_path, clip_text_embeddings)\n",
    "print(f\"\\nüíæ Saved: {output_path}\")\n",
    "print(f\"   Size: {output_path.stat().st_size / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1766134964534,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "T4VaN3eWqO-5",
    "outputId": "92aa65a6-3c58-4654-944f-fac8c5738441"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 9) COMBINE TEXT EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîó COMBINING TEXT EMBEDDINGS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Concatenate\n",
    "combined_text_embeddings = np.concatenate([mpnet_embeddings, clip_text_embeddings], axis=1)\n",
    "\n",
    "print(f\"‚úÖ Combined text embeddings\")\n",
    "print(f\"   mpnet: {mpnet_embeddings.shape}\")\n",
    "print(f\"   CLIP text: {clip_text_embeddings.shape}\")\n",
    "print(f\"   Combined: {combined_text_embeddings.shape}\")\n",
    "print(f\"   Expected: ({len(texts)}, {MODEL_CONFIG['text_combined_dim']})\")\n",
    "\n",
    "# Validate\n",
    "assert combined_text_embeddings.shape == (len(texts), MODEL_CONFIG['text_combined_dim'])\n",
    "assert not np.isnan(combined_text_embeddings).any()\n",
    "\n",
    "# Save\n",
    "output_path = EMB_TEXT_DIR / \"combined_1280d.npy\"\n",
    "np.save(output_path, combined_text_embeddings)\n",
    "print(f\"\\nüíæ Saved: {output_path}\")\n",
    "print(f\"   Size: {output_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# Free memory\n",
    "del mpnet_embeddings, clip_text_embeddings\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nüóëÔ∏è Cleared intermediate embeddings from memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396,
     "referenced_widgets": [
      "3a0333776ff84b61b564e17ce3e2764e",
      "b7c01a2785854f5888bcc59f8eab9d86",
      "4505c18505fc4e6abb207d8abaf692ad",
      "f1810e2c36374eec96212a880985bd59",
      "fc3bbc7205384250bc774bac63a2a743",
      "66280b06788d4952a1d945944952efb1",
      "9b9efaf9c5f448279a160fa48aa1b319",
      "98e5ced446dd42c69419ed0d542ff9e2",
      "185e5466a55645a3933a3e461850d71c",
      "2b079f70800b4cd381ee973109679d6a",
      "443d8f4ee0b547caa771f06e7bf3b8eb"
     ]
    },
    "executionInfo": {
     "elapsed": 560834,
     "status": "ok",
     "timestamp": 1766151292994,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "cvi0Kc3eqO-5",
    "outputId": "4a2b6317-7dcc-437e-fe65-4d2583b4c79a"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10) GENERATE IMAGE EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üñºÔ∏è GENERATING IMAGE EMBEDDINGS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find images directory\n",
    "OLD_PROJECT = Path(\"/content/drive/MyDrive/ai_fashion_assistant_v1\")\n",
    "possible_image_dirs = [\n",
    "    OLD_PROJECT / \"data/raw/images\",\n",
    "    PROJECT_ROOT / \"data/raw/images\",\n",
    "]\n",
    "\n",
    "IMAGES_DIR = None\n",
    "for img_dir in possible_image_dirs:\n",
    "    if img_dir.exists():\n",
    "        try:\n",
    "            # Test if readable\n",
    "            test_files = [f for f in os.listdir(img_dir) if f.endswith('.jpg')][:5]\n",
    "            if test_files:\n",
    "                IMAGES_DIR = img_dir\n",
    "                print(f\"‚úÖ Found images: {IMAGES_DIR}\")\n",
    "                break\n",
    "        except OSError:\n",
    "            continue\n",
    "\n",
    "if IMAGES_DIR is None:\n",
    "    raise FileNotFoundError(\"Images directory not found or not readable!\")\n",
    "\n",
    "print(f\"Total products: {len(df):,}\")\n",
    "print(f\"Batch size: {IMAGE_BATCH_SIZE}\")\n",
    "print(f\"Estimated time: {len(df) / IMAGE_BATCH_SIZE * 3 / 60:.1f} minutes\\n\")\n",
    "\n",
    "# Generate embeddings\n",
    "image_embeddings = []\n",
    "failed_images = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, len(df), IMAGE_BATCH_SIZE), desc=\"Image batches\"):\n",
    "    batch_df = df.iloc[i:i+IMAGE_BATCH_SIZE]\n",
    "    batch_images = []\n",
    "    batch_indices = []\n",
    "\n",
    "    # Load images\n",
    "    for idx, row in batch_df.iterrows():\n",
    "        img_path = IMAGES_DIR / f\"{row['id']}.jpg\"\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            batch_images.append(image)\n",
    "            batch_indices.append(idx)\n",
    "        except Exception as e:\n",
    "            failed_images.append((row['id'], str(e)))\n",
    "            # Use black image as placeholder\n",
    "            batch_images.append(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "            batch_indices.append(idx)\n",
    "\n",
    "    # Process batch\n",
    "    if batch_images:\n",
    "        inputs = clip_processor(images=batch_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Encode\n",
    "        with torch.no_grad():\n",
    "            batch_embs = clip_model.get_image_features(**inputs)\n",
    "            batch_embs = batch_embs.cpu().numpy()\n",
    "\n",
    "        image_embeddings.append(batch_embs)\n",
    "\n",
    "    # Clear GPU cache every 10 batches\n",
    "    if i % (IMAGE_BATCH_SIZE * 10) == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate\n",
    "image_embeddings = np.vstack(image_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Generated image embeddings\")\n",
    "print(f\"   Shape: {image_embeddings.shape}\")\n",
    "print(f\"   Expected: ({len(df)}, {MODEL_CONFIG['image_model_dim']})\")\n",
    "print(f\"   Time: {elapsed / 60:.1f} minutes\")\n",
    "print(f\"   Speed: {elapsed / len(df) * 1000:.1f}ms per image\")\n",
    "print(f\"   Failed: {len(failed_images)} images\")\n",
    "\n",
    "# Validate\n",
    "assert image_embeddings.shape == (len(df), MODEL_CONFIG['image_model_dim'])\n",
    "assert not np.isnan(image_embeddings).any(), \"NaN values detected!\"\n",
    "\n",
    "# Save\n",
    "output_path = EMB_IMAGE_DIR / \"clip_image_768d.npy\"\n",
    "np.save(output_path, image_embeddings)\n",
    "print(f\"\\nüíæ Saved: {output_path}\")\n",
    "print(f\"   Size: {output_path.stat().st_size / 1024**2:.1f} MB\")\n",
    "\n",
    "# Save failed images log\n",
    "if failed_images:\n",
    "    failed_log_path = EMB_IMAGE_DIR / \"failed_images.json\"\n",
    "    with open(failed_log_path, 'w') as f:\n",
    "        json.dump(failed_images, f, indent=2)\n",
    "    print(f\"\\nüìù Failed images log: {failed_log_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1766151293742,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "UmRlwAyrqO-6",
    "outputId": "e3c21784-9c98-43e7-bd32-612552d96954"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11) NORMALIZE EMBEDDINGS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìê NORMALIZING EMBEDDINGS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Normalize text embeddings\n",
    "print(\"Normalizing text embeddings...\")\n",
    "combined_text_normalized = normalize(combined_text_embeddings, norm='l2')\n",
    "output_path = EMB_TEXT_DIR / \"combined_1280d_normalized.npy\"\n",
    "np.save(output_path, combined_text_normalized)\n",
    "print(f\"‚úÖ Saved: {output_path} ({output_path.stat().st_size / 1024**2:.1f} MB)\")\n",
    "\n",
    "# Normalize image embeddings\n",
    "print(\"\\nNormalizing image embeddings...\")\n",
    "image_normalized = normalize(image_embeddings, norm='l2')\n",
    "output_path = EMB_IMAGE_DIR / \"clip_image_768d_normalized.npy\"\n",
    "np.save(output_path, image_normalized)\n",
    "print(f\"‚úÖ Saved: {output_path} ({output_path.stat().st_size / 1024**2:.1f} MB)\")\n",
    "\n",
    "print(\"\\n‚úÖ All embeddings normalized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1766151294284,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "S6cfJFXuqO-6",
    "outputId": "b6edb1ee-6d0f-4407-e423-03616e4fb95d"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 12) GENERATE EMBEDDING STATISTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä GENERATING EMBEDDING STATISTICS...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Statistics\n",
    "stats = {\n",
    "    \"total_products\": len(df),\n",
    "    \"text_embeddings\": {\n",
    "        \"mpnet\": {\n",
    "            \"shape\": list(mpnet_embeddings.shape) if 'mpnet_embeddings' in locals() else \"freed\",\n",
    "            \"dimension\": MODEL_CONFIG['text_model_primary_dim']\n",
    "        },\n",
    "        \"clip_text\": {\n",
    "            \"shape\": list(clip_text_embeddings.shape) if 'clip_text_embeddings' in locals() else \"freed\",\n",
    "            \"dimension\": MODEL_CONFIG['text_model_secondary_dim']\n",
    "        },\n",
    "        \"combined\": {\n",
    "            \"shape\": list(combined_text_embeddings.shape),\n",
    "            \"dimension\": MODEL_CONFIG['text_combined_dim'],\n",
    "            \"mean_norm\": float(np.linalg.norm(combined_text_embeddings, axis=1).mean()),\n",
    "            \"std_norm\": float(np.linalg.norm(combined_text_embeddings, axis=1).std())\n",
    "        }\n",
    "    },\n",
    "    \"image_embeddings\": {\n",
    "        \"shape\": list(image_embeddings.shape),\n",
    "        \"dimension\": MODEL_CONFIG['image_model_dim'],\n",
    "        \"mean_norm\": float(np.linalg.norm(image_embeddings, axis=1).mean()),\n",
    "        \"std_norm\": float(np.linalg.norm(image_embeddings, axis=1).std()),\n",
    "        \"failed_count\": len(failed_images)\n",
    "    },\n",
    "    \"files\": {\n",
    "        \"text\": [\n",
    "            \"mpnet_768d.npy\",\n",
    "            \"clip_text_512d.npy\",\n",
    "            \"combined_1280d.npy\",\n",
    "            \"combined_1280d_normalized.npy\"\n",
    "        ],\n",
    "        \"image\": [\n",
    "            \"clip_image_768d.npy\",\n",
    "            \"clip_image_768d_normalized.npy\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save stats\n",
    "stats_path = EMB_DIR / \"embedding_stats.json\"\n",
    "with open(stats_path, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Stats saved: {stats_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total products: {stats['total_products']:,}\")\n",
    "print(f\"\\nText embeddings:\")\n",
    "print(f\"  Combined shape: {stats['text_embeddings']['combined']['shape']}\")\n",
    "print(f\"  Mean norm: {stats['text_embeddings']['combined']['mean_norm']:.4f}\")\n",
    "print(f\"\\nImage embeddings:\")\n",
    "print(f\"  Shape: {stats['image_embeddings']['shape']}\")\n",
    "print(f\"  Mean norm: {stats['image_embeddings']['mean_norm']:.4f}\")\n",
    "print(f\"  Failed: {stats['image_embeddings']['failed_count']}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1766151294358,
     "user": {
      "displayName": "Hatice Baydemir",
      "userId": "09255724962739063380"
     },
     "user_tz": -180
    },
    "id": "q6215NsIqO-7",
    "outputId": "2269ba89-f347-49bd-d527-af1bd0eeadea"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 13) QUALITY GATES VALIDATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüéØ QUALITY GATES VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gates_passed = True\n",
    "\n",
    "# Gate 1: All embeddings generated\n",
    "if combined_text_embeddings.shape[0] == len(df) and image_embeddings.shape[0] == len(df):\n",
    "    print(f\"‚úÖ Gate 1: All embeddings generated ({len(df):,} products)\")\n",
    "else:\n",
    "    print(f\"‚ùå Gate 1: Embedding count mismatch!\")\n",
    "    gates_passed = False\n",
    "\n",
    "# Gate 2: No NaN values\n",
    "text_has_nan = np.isnan(combined_text_embeddings).any()\n",
    "image_has_nan = np.isnan(image_embeddings).any()\n",
    "\n",
    "if not text_has_nan and not image_has_nan:\n",
    "    print(\"‚úÖ Gate 2: No NaN values detected\")\n",
    "else:\n",
    "    print(f\"‚ùå Gate 2: NaN values found! (text: {text_has_nan}, image: {image_has_nan})\")\n",
    "    gates_passed = False\n",
    "\n",
    "# Gate 3: Dimensions correct\n",
    "text_dim_ok = combined_text_embeddings.shape[1] == MODEL_CONFIG['text_combined_dim']\n",
    "image_dim_ok = image_embeddings.shape[1] == MODEL_CONFIG['image_model_dim']\n",
    "\n",
    "if text_dim_ok and image_dim_ok:\n",
    "    print(f\"‚úÖ Gate 3: Dimensions correct (text: {MODEL_CONFIG['text_combined_dim']}d, image: {MODEL_CONFIG['image_model_dim']}d)\")\n",
    "else:\n",
    "    print(f\"‚ùå Gate 3: Dimension mismatch!\")\n",
    "    gates_passed = False\n",
    "\n",
    "# Gate 4: File sizes reasonable\n",
    "text_file = EMB_TEXT_DIR / \"combined_1280d.npy\"\n",
    "image_file = EMB_IMAGE_DIR / \"clip_image_768d.npy\"\n",
    "\n",
    "text_size_mb = text_file.stat().st_size / 1024**2\n",
    "image_size_mb = image_file.stat().st_size / 1024**2\n",
    "total_size_gb = (text_size_mb + image_size_mb) / 1024\n",
    "\n",
    "if 1 < total_size_gb < 5:  # Reasonable range\n",
    "    print(f\"‚úÖ Gate 4: File sizes reasonable ({total_size_gb:.2f} GB total)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Gate 4: File size unusual ({total_size_gb:.2f} GB)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if gates_passed:\n",
    "    print(\"\\nüéâ ALL QUALITY GATES PASSED!\")\n",
    "    print(\"‚úÖ Embeddings ready for hybrid space creation!\")\n",
    "    print(\"\\nüìç Next: 03_hybrid_space_creation.ipynb\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è SOME QUALITY GATES FAILED!\")\n",
    "    print(\"   Please review and fix before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIthOHwOqO-7"
   },
   "source": [
    "---\n",
    "\n",
    "## üìã Summary\n",
    "\n",
    "**Embeddings Generated:**\n",
    "- ‚úÖ Text (mpnet): 44,418 x 768d\n",
    "- ‚úÖ Text (CLIP): 44,418 x 512d\n",
    "- ‚úÖ Text (combined): 44,418 x 1280d\n",
    "- ‚úÖ Image (CLIP): 44,418 x 768d\n",
    "- ‚úÖ All normalized versions\n",
    "\n",
    "**Files Created:**\n",
    "- `embeddings/text/mpnet_768d.npy`\n",
    "- `embeddings/text/clip_text_512d.npy`\n",
    "- `embeddings/text/combined_1280d.npy`\n",
    "- `embeddings/text/combined_1280d_normalized.npy`\n",
    "- `embeddings/image/clip_image_768d.npy`\n",
    "- `embeddings/image/clip_image_768d_normalized.npy`\n",
    "- `embeddings/embedding_stats.json`\n",
    "\n",
    "**Total Size:** ~2-3 GB\n",
    "\n",
    "**Next Notebook:** `03_hybrid_space_creation.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDMK2WYo4kph"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
