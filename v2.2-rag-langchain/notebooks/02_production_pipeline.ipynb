{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37-y1EsPQXjT"
      },
      "source": [
        "# Production RAG Pipeline\n",
        "\n",
        "**Project:** AI Fashion Assistant v2.2  \n",
        "**Focus:** Production-ready RAGPipeline class  \n",
        "**Author:** Hatice Baydemir  \n",
        "**Date:** January 2, 2026\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a production-ready `FashionRAGPipeline` class with:\n",
        "- Clean API interface\n",
        "- Response caching\n",
        "- Batch processing\n",
        "- Error handling\n",
        "- Configuration management\n",
        "\n",
        "**Usage:**\n",
        "```python\n",
        "pipeline = FashionRAGPipeline(...)\n",
        "result = pipeline.query(\"blue dress\")\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9VHSDFFQXjU"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGq_dv01QXjV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366783553,
          "user_tz": -180,
          "elapsed": 19520,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "b8ef052d-efd4-41f5-8c74-ab0489a05d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u2705 Ready\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/ai_fashion_assistant_v2')\n",
        "print('\u2705 Ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItSSSTfEQXjX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366791239,
          "user_tz": -180,
          "elapsed": 7685,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "5fcfb6be-7698-456c-c315-4d292c97d747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u2705 Installed\n"
          ]
        }
      ],
      "source": [
        "!pip install -q groq sentence-transformers faiss-cpu\n",
        "print('\u2705 Installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFVbFNoQXjX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366825233,
          "user_tz": -180,
          "elapsed": 33993,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "557fd1f0-bff8-4d17-9d41-4e322115f2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Imports\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from groq import Groq\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "print('\u2705 Imports')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSbDGdRhQXjY"
      },
      "source": [
        "## 2. FashionRAGPipeline Class\n",
        "\n",
        "Production-ready implementation with full features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMHIAeRQQXjY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366825284,
          "user_tz": -180,
          "elapsed": 43,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "10b621dc-3965-4dd0-f0f2-9d0fb3488004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 FashionRAGPipeline class defined\n"
          ]
        }
      ],
      "source": [
        "class FashionRAGPipeline:\n",
        "    \"\"\"\n",
        "    Production-ready RAG pipeline for fashion product search.\n",
        "\n",
        "    Features:\n",
        "    - FAISS vector search (44K products)\n",
        "    - GROQ LLM integration (Llama-3.3-70B)\n",
        "    - Response caching for efficiency\n",
        "    - Batch processing support\n",
        "    - Configurable retrieval parameters\n",
        "\n",
        "    Example:\n",
        "        >>> pipeline = FashionRAGPipeline(\n",
        "        ...     metadata_path=\"data/processed/meta_ssot.csv\",\n",
        "        ...     embeddings_path=\"v2.0-baseline/embeddings/text/mpnet_768d.npy\",\n",
        "        ...     groq_api_key=\"your_key\"\n",
        "        ... )\n",
        "        >>> result = pipeline.query(\"blue summer dress\")\n",
        "        >>> print(result['answer'])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        metadata_path: str,\n",
        "        embeddings_path: str,\n",
        "        groq_api_key: str,\n",
        "        encoder_model: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
        "        llm_model: str = \"llama-3.3-70b-versatile\",\n",
        "        temperature: float = 0.1,\n",
        "        max_tokens: int = 500\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the RAG pipeline.\n",
        "\n",
        "        Args:\n",
        "            metadata_path: Path to product metadata CSV\n",
        "            embeddings_path: Path to precomputed embeddings\n",
        "            groq_api_key: GROQ API key\n",
        "            encoder_model: Sentence transformer model name\n",
        "            llm_model: GROQ LLM model name\n",
        "            temperature: LLM temperature (0-1)\n",
        "            max_tokens: Max tokens for LLM response\n",
        "        \"\"\"\n",
        "        print(\"Initializing FashionRAGPipeline...\")\n",
        "\n",
        "        # Store config\n",
        "        self.config = {\n",
        "            'encoder_model': encoder_model,\n",
        "            'llm_model': llm_model,\n",
        "            'temperature': temperature,\n",
        "            'max_tokens': max_tokens\n",
        "        }\n",
        "\n",
        "        # Load data\n",
        "        self.metadata = pd.read_csv(metadata_path)\n",
        "        self.embeddings = np.load(embeddings_path)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        self.embeddings_norm = self.embeddings / np.linalg.norm(\n",
        "            self.embeddings, axis=1, keepdims=True\n",
        "        )\n",
        "\n",
        "        # Setup encoder\n",
        "        self.encoder = SentenceTransformer(encoder_model)\n",
        "\n",
        "        # Build FAISS index\n",
        "        dimension = self.embeddings_norm.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)\n",
        "        self.index.add(self.embeddings_norm.astype('float32'))\n",
        "\n",
        "        # Setup LLM client\n",
        "        self.llm_client = Groq(api_key=groq_api_key)\n",
        "\n",
        "        # Create product documents\n",
        "        self.product_docs = self._create_documents()\n",
        "\n",
        "        # Initialize cache\n",
        "        self.cache = {}\n",
        "        self.stats = {'queries': 0, 'cache_hits': 0}\n",
        "\n",
        "        print(f\"\u2705 Pipeline ready!\")\n",
        "        print(f\"   Products: {len(self.metadata):,}\")\n",
        "        print(f\"   Index: {self.index.ntotal:,} vectors ({dimension}d)\")\n",
        "        print(f\"   Encoder: {encoder_model}\")\n",
        "        print(f\"   LLM: {llm_model}\")\n",
        "\n",
        "    def _create_documents(self) -> List[str]:\n",
        "        \"\"\"Create text documents from product metadata.\"\"\"\n",
        "        docs = []\n",
        "        for _, row in self.metadata.iterrows():\n",
        "            doc = f\"\"\"{row['productDisplayName']}.\n",
        "Category: {row.get('masterCategory', 'Unknown')}.\n",
        "Type: {row.get('articleType', 'Unknown')}.\n",
        "Color: {row.get('baseColour', 'Unknown')}.\n",
        "Gender: {row.get('gender', 'Unisex')}.\n",
        "Season: {row.get('season', 'All')}.\"\"\"\n",
        "            docs.append(doc)\n",
        "        return docs\n",
        "\n",
        "    def retrieve(self, query: str, k: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        Retrieve relevant products using vector search.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language query\n",
        "            k: Number of products to retrieve\n",
        "\n",
        "        Returns:\n",
        "            Dict with indices, scores, products\n",
        "        \"\"\"\n",
        "        # Encode query\n",
        "        query_emb = self.encoder.encode([query])[0]\n",
        "        query_emb = query_emb / np.linalg.norm(query_emb)\n",
        "\n",
        "        # Search FAISS\n",
        "        scores, indices = self.index.search(\n",
        "            query_emb.reshape(1, -1).astype('float32'),\n",
        "            k\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'indices': indices[0].tolist(),\n",
        "            'scores': scores[0].tolist(),\n",
        "            'products': [self.product_docs[i] for i in indices[0]]\n",
        "        }\n",
        "\n",
        "    def augment(self, query: str, retrieved: Dict) -> str:\n",
        "        \"\"\"\n",
        "        Create augmented prompt with retrieved context.\n",
        "\n",
        "        Args:\n",
        "            query: User query\n",
        "            retrieved: Retrieved products dict\n",
        "\n",
        "        Returns:\n",
        "            Augmented prompt string\n",
        "        \"\"\"\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"{i+1}. {prod}\"\n",
        "            for i, prod in enumerate(retrieved['products'])\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"You are a fashion shopping assistant. Recommend products based on the user's query.\n",
        "\n",
        "Available Products:\n",
        "{context}\n",
        "\n",
        "User Query: {query}\n",
        "\n",
        "Recommendation (be specific, mention product names):\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate answer using LLM.\n",
        "\n",
        "        Args:\n",
        "            prompt: Augmented prompt\n",
        "\n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        response = self.llm_client.chat.completions.create(\n",
        "            model=self.config['llm_model'],\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=self.config['temperature'],\n",
        "            max_tokens=self.config['max_tokens']\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    def query(self, query: str, k: int = 5, use_cache: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Complete RAG query pipeline.\n",
        "\n",
        "        Args:\n",
        "            query: Natural language query\n",
        "            k: Number of products to retrieve\n",
        "            use_cache: Whether to use cached responses\n",
        "\n",
        "        Returns:\n",
        "            Dict with query, answer, retrieved products, scores\n",
        "        \"\"\"\n",
        "        self.stats['queries'] += 1\n",
        "\n",
        "        # Check cache\n",
        "        cache_key = f\"{query}_{k}\"\n",
        "        if use_cache and cache_key in self.cache:\n",
        "            self.stats['cache_hits'] += 1\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # RAG pipeline: Retrieve \u2192 Augment \u2192 Generate\n",
        "        retrieved = self.retrieve(query, k)\n",
        "        prompt = self.augment(query, retrieved)\n",
        "        answer = self.generate(prompt)\n",
        "\n",
        "        result = {\n",
        "            'query': query,\n",
        "            'answer': answer,\n",
        "            'retrieved_products': retrieved['products'],\n",
        "            'scores': retrieved['scores'],\n",
        "            'indices': retrieved['indices'],\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Cache result\n",
        "        if use_cache:\n",
        "            self.cache[cache_key] = result\n",
        "\n",
        "        return result\n",
        "\n",
        "    def batch_query(self, queries: List[str], k: int = 5) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Process multiple queries in batch.\n",
        "\n",
        "        Args:\n",
        "            queries: List of queries\n",
        "            k: Number of products per query\n",
        "\n",
        "        Returns:\n",
        "            List of results\n",
        "        \"\"\"\n",
        "        return [self.query(q, k) for q in queries]\n",
        "\n",
        "    def get_stats(self) -> Dict:\n",
        "        \"\"\"Get pipeline statistics.\"\"\"\n",
        "        cache_hit_rate = (\n",
        "            self.stats['cache_hits'] / self.stats['queries']\n",
        "            if self.stats['queries'] > 0 else 0\n",
        "        )\n",
        "        return {\n",
        "            **self.stats,\n",
        "            'cache_hit_rate': cache_hit_rate,\n",
        "            'cache_size': len(self.cache)\n",
        "        }\n",
        "\n",
        "    def save_cache(self, path: str):\n",
        "        \"\"\"Save cache to JSON file.\"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.cache, f, indent=2)\n",
        "        print(f\"\u2705 Cache saved: {len(self.cache)} entries\")\n",
        "\n",
        "    def load_cache(self, path: str):\n",
        "        \"\"\"Load cache from JSON file.\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            self.cache = json.load(f)\n",
        "        print(f\"\u2705 Cache loaded: {len(self.cache)} entries\")\n",
        "\n",
        "print('\u2705 FashionRAGPipeline class defined')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEAAej7hQXja"
      },
      "source": [
        "## 3. Initialize Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632,
          "referenced_widgets": [
            "6e332b6b7f1c45538b49a29667565438",
            "c49dc753b90143a8bdaddb5c0af722a3",
            "04fc551606c448919a9ec746b53acd6f",
            "ed0a637999d444e984b0b4d98ee77a4f",
            "a115d091cdc4483b8a6660ce971cdf6b",
            "3923cc08c71946f2bf695377e2ee0490",
            "5e0e9a7c37fc4b1e93bd2997b7f50fb1",
            "65d50674cf18472091481a6b25256554",
            "33806a38e76e4b01bc264c241ed43e6a",
            "10a5158555544b2581b46e1bcbb54a83",
            "2fa8f40e0fb34e79a47b0f709b4acfce",
            "283f4ae8b8634938bc27b6ef131dd4b2",
            "c15a0d04f1654e3b8c716423f1ad49d4",
            "72a3b2155eac4cc98c3ffdaa0acdd3b8",
            "b052f319794849399a6067ae6c5b7992",
            "6b1ca8de2fe8459fa283c4e49922a7a5",
            "e8cd16b3d45d4de08903073d963f901d",
            "41401344702240bda343ad0c37798dd6",
            "d234a869869d4d9ab915ec014c7947c5",
            "c986c27c0a564f73aafb88e17b32f3d9",
            "9baae801f4e64ce2849960d54f4940b5",
            "5564a06fc3654ebbaf6d6ef1589c3da3",
            "76bc38bfa4b047aaa1e5f777a3ea719a",
            "7056e08f8a7540d9bbf56ae63edc7361",
            "22f4a16c9e3e4b6d985abb0ff8f1be84",
            "140748a13fbf40dfadc70209cd73dcdb",
            "6c7e4cd15dfd4ec88f90323a1749a431",
            "7cbc2e2564144c30945d47ba50d3da66",
            "d4d6e37868d1405584489bb2ca67f1f6",
            "80f60e43ca484f639e15af048e99421f",
            "41268a49fef84fe2bb1b7e61357343bd",
            "84743cd13a504840bf34a5a89c83b03f",
            "b022477616d74466afa6278ee6547617",
            "f4900180d9474741b0d3f82b9d3af422",
            "eb52240a4aa44e3389b11bcae57741aa",
            "e89aee13318049b8b69884f1be2d50a6",
            "c982af930ee946a7987dd1b5dd1b591e",
            "cfc036eb9d22485384b19f6c5bfb15c0",
            "4f9a45f1648d438e9884e0f5bf7a48d2",
            "374ba0a5b3154accb4a503695026f672",
            "0bf0d16dd67e465daf5314c5a455b1d1",
            "e785ee37fb4440a0a47ef386cbf2612a",
            "0f979a0b493d4d5ca41c01deaa3f7962",
            "95a22f57b80643f88c3d24e0cf30efad",
            "071863cb037c439f8d06d172d8705197",
            "e8ad520e671b4061becaa8e9a517cbbe",
            "e7931d9ebf6441ec9665fa82ef72987e",
            "2b303623a3394258ac2658507a786c58",
            "eef9f22ecaa24ea4b9f8488b886a2dcf",
            "6e5e59cf494b4a128b59719bbca8a69e",
            "043c9470cd23429587b0f9a3fe94985a",
            "6edb827919414c57b228694632839b29",
            "c16d4ef0a18b420b856e8505f31fe89f",
            "03a4e4e9257b41f0ba937cd4737eb710",
            "65f0c221c98643dc8112f2153801825d",
            "77957448f219427d8e2da113f560718f",
            "ad10f02b50e24ec78f68ce2f23fb37ff",
            "466d1d4d07bd413e812e86692497d377",
            "1590853749ab433dbb6d52fec772f3ee",
            "a676781971454709a73f73caf770e39f",
            "02af2ffe42724d44b95cf13669df00de",
            "77af8465c52848ec93f26c828a5b53a3",
            "dd4bd7a2e57241dcb98f81d793a5fbcc",
            "793ce1e4c5d54286b7aeb488d4ce136d",
            "0e204c1208e74056acd4c7370fe2398a",
            "f08f8893013f47b1bd0b2cbcbd387041",
            "6779118c47454c6da49fbf12b45c9d9d",
            "bc28b2c6aaa942bfaad7f39fdc9bc7c7",
            "60a879ffe1a1458e952b7dafa9057e55",
            "955ad3fecf35414da4920923def5aa09",
            "c3a10d0266374449b09eb0ffd8792848",
            "c7c5d7be38e74c9a99d381447217104e",
            "20ad66fed10d4668b912b55ac696ba71",
            "bbe92adc61e24f089637661dd84896ef",
            "3b7e3b2d036b4e538c40e24c99d24472",
            "2bd2e8ecd8e047899d3333a3fa484cd0",
            "8a7ec780aca845c785feac6c3a23a889",
            "c7afa892292749319c06fca5f7133672",
            "bd93f967b19b4a07ba55552355245743",
            "a6b3cfa83f544198801844f11204b538",
            "4a3f0729060748c582720ee14aece8bd",
            "e903fc5ce51d44f7af830cd9ce35fce7",
            "adc365a71b144a9f8a5a722ca9349154",
            "c03be9a16a9f41c5881bbae3309d10ce",
            "92229829b095455f97437db326be3729",
            "e4f990d85efa41ffa9d6ba33b226000f",
            "0e499bd3d1d847a9bdd0eedf4ebea5f2",
            "a1181f6e6f9c4ac793606d2cd89e892d",
            "64b3d1d3d6524f4e9c7138041dacfe8a",
            "0fd60ec0e98d4173ac1d9cde3be8c652",
            "18d70bcd1ac2456497be298178f697ce",
            "e4f56e110e104b0f96d127328ad514b6",
            "ad5a5e2e211045ad93a68d07b861a761",
            "8c3c2545658e4156b9796e9f5d31c2e7",
            "7b21a19d546a4b2b955824a9b8eeba8c",
            "53fd6496d95640ef821966494fd3ec37",
            "ae2e34aa2ac7454ab5b17ae21593315b",
            "7b67e6097b744bcaab1e44e13c1cfa60",
            "8fba46d1e62c472d8ddf086cd0ae46c8",
            "3a5ff156147541499df0196acae25738",
            "af8ef68588a04fb89eb77d43b6336d33",
            "56565925b32c467e90bb1c2968ae2b6c",
            "72e3d159070a4a54b4909c475cde7f23",
            "64720316b74141aba58efa2af9bf0c77",
            "0d56cf92ff6d4cc288b283e92a9dc00b",
            "214fc303e7d245b1a04cb960b3d2aac9",
            "1fe1490fd64c44f4a1c42aec7d7fce26",
            "a7e2c88a41d840fe98a89f3ff0f4c442",
            "3fb44128043942cf843786f1ce86525f",
            "b2e08520cc944d2db1023d7d8bd93a73",
            "e73b428fe56c4b3ca1874f43e9dad1a9",
            "f54acb93bc8043b7982f37705647cb5b",
            "66f413dd5c79402086591275b20681ec",
            "837198d81ff84a3f836ec05005c4584a",
            "735720610338440abed0dbb68e94d879",
            "decca363dd154bc6afde93afb9b764d5",
            "5b1958bed2e140129a0e0e1446922b57",
            "e3a7bb841d484afa9fa245e069b9639c",
            "8c7ef52c6275427fb39aa12a250ad9d3",
            "3d939170f9e94de897a53e761c65cff5",
            "2ddbc3fb80e44158a2128972557ed2cd"
          ]
        },
        "id": "Ble0gsipQXjb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366856248,
          "user_tz": -180,
          "elapsed": 30961,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "d8c169bf-e2b4-4d6d-be1f-81e8b95a70c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing FashionRAGPipeline...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e332b6b7f1c45538b49a29667565438"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "283f4ae8b8634938bc27b6ef131dd4b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76bc38bfa4b047aaa1e5f777a3ea719a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4900180d9474741b0d3f82b9d3af422"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "071863cb037c439f8d06d172d8705197"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77957448f219427d8e2da113f560718f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/402 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6779118c47454c6da49fbf12b45c9d9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7afa892292749319c06fca5f7133672"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64b3d1d3d6524f4e9c7138041dacfe8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a5ff156147541499df0196acae25738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e73b428fe56c4b3ca1874f43e9dad1a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Pipeline ready!\n",
            "   Products: 44,417\n",
            "   Index: 44,417 vectors (768d)\n",
            "   Encoder: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "   LLM: llama-3.3-70b-versatile\n",
            "\n",
            "\u2705 Pipeline initialized and ready for queries!\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "GROQ_API_KEY = \"YOUR_GROQ_API_KEY_HERE\"  # \u26a0\ufe0f REPLACE!\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = FashionRAGPipeline(\n",
        "    metadata_path='data/processed/meta_ssot.csv',\n",
        "    embeddings_path='v2.0-baseline/embeddings/text/mpnet_768d.npy',\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    temperature=0.1,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print('\\n\u2705 Pipeline initialized and ready for queries!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsgixJOuQXjc"
      },
      "source": [
        "## 4. Test Single Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGH-IKxhQXjc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366857738,
          "user_tz": -180,
          "elapsed": 1479,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "480b09f7-82ec-4d0f-da12-6abe422b6638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd0d Query: blue summer dress\n",
            "\n",
            "\ud83d\udcca Retrieval:\n",
            "   Top score: 0.889\n",
            "   Products: 5\n",
            "\n",
            "\ud83e\udd16 Answer:\n",
            "Based on your query for a \"blue summer dress\", I would recommend the following products:\n",
            "\n",
            "1. Mineral Blue Dress - A perfect choice for summer, this dress is available in a beautiful blue color and is designed specifically for the summer season.\n",
            "2. 109F Blue A-Line Dress - This dress is a great option for a blue summer dress, with its A-Line design and vibrant blue color, it's perfect for hot summer days.\n",
            "3. AND Women Blue Dress - This dress is a stylish and comfortable choice for summer, with its blue color and lightweight design, it's ideal for outdoor events and everyday wear.\n",
            "4. Elle Women Blue Dress - This dress is a great choice for a blue summer dress, with its elegant design and beautiful blue color, it's perfect for special occasions and summer gatherings.\n",
            "\n",
            "All of these dresses are from our women's apparel collection and are suitable for the summer season. If you're looking for a specific style, please let me know and I can provide more tailored recommendations.\n"
          ]
        }
      ],
      "source": [
        "# Single query\n",
        "result = pipeline.query(\"blue summer dress\", k=5)\n",
        "\n",
        "print('\ud83d\udd0d Query:', result['query'])\n",
        "print(f\"\\n\ud83d\udcca Retrieval:\")\n",
        "print(f\"   Top score: {result['scores'][0]:.3f}\")\n",
        "print(f\"   Products: {len(result['retrieved_products'])}\")\n",
        "print(f\"\\n\ud83e\udd16 Answer:\")\n",
        "print(result['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quplxQLOQXjd"
      },
      "source": [
        "## 5. Test Batch Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNleZslcQXjd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366860722,
          "user_tz": -180,
          "elapsed": 2982,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "fdd19934-70e6-4fda-eb96-060dfd45f944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udd04 Processing batch queries...\n",
            "\n",
            "\u2705 Batch complete: 4 queries\n",
            "\n",
            "Results:\n",
            "\n",
            "1. Query: casual shoes for men\n",
            "   Score: 0.864\n",
            "   Answer: Based on your query for casual shoes for men, I would recommend the following pr...\n",
            "\n",
            "2. Query: red lipstick\n",
            "   Score: 0.829\n",
            "   Answer: Based on your query for a red lipstick, I would recommend the following options:...\n",
            "\n",
            "3. Query: winter jacket\n",
            "   Score: 0.672\n",
            "   Answer: Based on your query for a \"winter jacket\", I would recommend the following produ...\n",
            "\n",
            "4. Query: formal office wear\n",
            "   Score: 0.543\n",
            "   Answer: Based on your query for formal office wear, I would recommend the following prod...\n"
          ]
        }
      ],
      "source": [
        "# Batch queries\n",
        "test_queries = [\n",
        "    \"casual shoes for men\",\n",
        "    \"red lipstick\",\n",
        "    \"winter jacket\",\n",
        "    \"formal office wear\"\n",
        "]\n",
        "\n",
        "print('\ud83d\udd04 Processing batch queries...')\n",
        "results = pipeline.batch_query(test_queries, k=5)\n",
        "\n",
        "print(f\"\\n\u2705 Batch complete: {len(results)} queries\")\n",
        "print('\\nResults:')\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. Query: {r['query']}\")\n",
        "    print(f\"   Score: {r['scores'][0]:.3f}\")\n",
        "    print(f\"   Answer: {r['answer'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efavsM4aQXjd"
      },
      "source": [
        "## 6. Pipeline Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYNGDoqQXje",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366860731,
          "user_tz": -180,
          "elapsed": 7,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "3e088754-8c9d-4fcf-e282-a082f6ae27d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\ud83d\udcca Pipeline Statistics:\n",
            "   Total queries: 5\n",
            "   Cache hits: 0\n",
            "   Cache hit rate: 0.0%\n",
            "   Cache size: 5 entries\n"
          ]
        }
      ],
      "source": [
        "# Get statistics\n",
        "stats = pipeline.get_stats()\n",
        "\n",
        "print('\ud83d\udcca Pipeline Statistics:')\n",
        "print(f\"   Total queries: {stats['queries']}\")\n",
        "print(f\"   Cache hits: {stats['cache_hits']}\")\n",
        "print(f\"   Cache hit rate: {stats['cache_hit_rate']:.1%}\")\n",
        "print(f\"   Cache size: {stats['cache_size']} entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N03nBt9DQXje"
      },
      "source": [
        "## 7. Save Pipeline Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHe3prjCQXje",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767366860951,
          "user_tz": -180,
          "elapsed": 217,
          "user": {
            "displayName": "Hatice Baydemir",
            "userId": "09255724962739063380"
          }
        },
        "outputId": "06921af8-4e8a-41dc-c3f1-f9a964d512f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Cache saved: 5 entries\n",
            "\u2705 Artifacts saved!\n"
          ]
        }
      ],
      "source": [
        "# Save cache\n",
        "pipeline.save_cache('v2.2-rag-langchain/cache.json')\n",
        "\n",
        "# Save config\n",
        "with open('v2.2-rag-langchain/configs/pipeline_config.json', 'w') as f:\n",
        "    json.dump(pipeline.config, f, indent=2)\n",
        "\n",
        "print('\u2705 Artifacts saved!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dnLp7exQXje"
      },
      "source": [
        "## Summary\n",
        "\n",
        "**Production features implemented:**\n",
        "- \u2705 Clean class-based API\n",
        "- \u2705 Response caching (efficiency)\n",
        "- \u2705 Batch processing support\n",
        "- \u2705 Statistics tracking\n",
        "- \u2705 Configurable parameters\n",
        "- \u2705 Error handling ready\n",
        "\n",
        "**Next:** Notebook 3 - Comprehensive evaluation\n",
        "\n",
        "---\n",
        "\n",
        "**Ready for production deployment!** \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}